[["index.html", "Hydrological Drought (2nd Edition) Preface", " Hydrological Drought (2nd Edition) Jim Stagge Preface This website provides all worked examples associated with the 2nd Edition of Hydrologic Drought: Processes and Estimation Methods for Streamflow and Groundwater (Tallaksen and van Lanen). All code is written in the R programming language. "],["chapter-5.html", "Chapter 5 Worked Example 5.1: Flow Duration Curve Worked example 5.2: Mean annual minimum n-day flow Worked Example 5.3: Base Flow Index Worked example 5.4: Threshold level method Worked example 5.5: Sequent Peak Algorithm Worked example 5.6, Example of how to estimate SGI using data from Stonor Park, UK Worked example 5.7: Rank and correlation coefficients Worked example 5.8: No-flow indices", " Chapter 5 Worked Example 5.1: Flow Duration Curve Loading the Data In this example we are going to use river flow data from the river Ngaruroro which is part of the International Data Set in the package hydroDrought. The dataset becomes accessible by loading the hydroDrought package. library(tidyverse) library(hydroDrought) ngaruroro &lt;- international %&gt;% filter(river == &quot;Ngaruroro&quot;) %&gt;% dplyr::select(data) %&gt;% unnest(data) The complete record (20 September 1963 to 8 October 2019) of daily data from River Ngaruroro at Kuripapango (NZ) are used here to construct a flow duration curve (FDC) based on a daily time step, \\(\\Delta t = 1\\) day. The total number of \\(\\Delta t\\) intervals is \\(N = 20473\\) days. Table 5.1 lists the first ten flow values. The first three columns show the date and the corresponding riverflow value, \\(Q\\). Calculation of the FDC The flow duration curve is constructed following the calculation steps as seen in the right part of the Table 5.1: The rank, \\(i\\), of each value (column four in Table 5.1) is calculated (using the rank() function), which means that if the list is sorted, the rank will be its position. Here the series is sorted in descending order and the \\(i^{th}\\) largest value has rank \\(i\\) (i.e. the largest value has rank 1). The exceedance frequency (column five in Table 5.1), \\(EF_{Q_i}\\) is calculated as: \\[EF_{Q_i} = \\frac{i} {N}\\] which gives an estimate of the empirical exceedance frequency of the \\(i^{th}\\) largest event. \\(EF_{Q_i}\\) designates here the observed frequency when the flow, \\(Q\\), is larger than the flow value with rank \\(i\\), \\(Q_i\\) . exceedance_frequency &lt;- function(flow) { # current rank i &lt;- rank(-flow, ties.method = &quot;min&quot;, na.last = &quot;keep&quot;) # largest rank in sample (= number of non-missing values) N &lt;- length(na.omit(flow)) # the exceedance frequency can be seen as the relative rank return(i / N) } ngaruroro &lt;- ngaruroro %&gt;% mutate( rank = rank(-discharge, ties.method = &quot;min&quot;), freq.exc = exceedance_frequency(discharge) ) ⊕Table 5.1 Calculation of daily flow duration curve for River Ngaruroro at Kuripapango, NZ ## # A tibble: 7 x 4 ## time discharge rank freq.exc ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1963-09-20 30.5 2501 0.124 ## 2 1963-09-21 52.8 827 0.0409 ## 3 1963-09-22 43.6 1228 0.0607 ## 4 1963-09-23 37.3 1686 0.0834 ## 5 1963-09-24 32.3 2240 0.111 ## 6 1963-09-25 29.0 2736 0.135 ## 7 1963-09-26 25.3 3482 0.172 Plot of the FDC The sorted table columns are then plotted (Figure 5.2). The ordinate axis is here logarithmic. ggplot(ngaruroro, aes(x = freq.exc * 100, y = discharge)) + geom_line() + scale_y_log10(expand = expansion()) + scale_x_continuous(expand = expansion()) + labs(x = &quot;Exceedance Frequency (%)&quot;, y = expression(paste(&quot;Flow (&quot;, m^{3}, s^{-1}, &quot;)&quot;))) + theme(plot.margin = unit(c(0, 10, 0, 0), units = &quot;pt&quot;)) Figure 1: Figure 5.2 Flow duration curve for River Ngaruroro at Kuripapango, NZ. Selected exceedance values Values for a particular frequency, for example the 90-percentile (\\(Q_{90}\\)), can be obtained as the value of \\(Q\\) corresponding to the largest value of \\(EF_{Q_i}\\) that is less than or equal to the value of \\(EF_{Q_i}\\) sought for. A sample of corresponding values in this range is shown in Table 5.2, and the 90-percentile flow value is taken as 4.9 m3s-1. Alternatively, in case of large differences between successive values, a linear interpolation can be used. ⊕Table 5.2 An extract of values corresponding to \\(Q_{90}\\). ## # A tibble: 9 x 4 ## time discharge rank freq.exc ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2001-03-03 4.94 18202 0.900 ## 2 1999-01-16 4.94 18203 0.900 ## 3 2005-11-24 4.94 18203 0.900 ## 4 2013-01-18 4.94 18203 0.900 ## 5 1990-12-29 4.93 18206 0.900 ## 6 1968-02-17 4.93 18207 0.900 ## 7 1994-03-28 4.93 18207 0.900 ## 8 1998-03-29 4.93 18207 0.900 ## 9 2002-03-26 4.93 18207 0.900 Fast-Track The function lfquantile() calculates percentiles (quantiles) directly. The exact (interpolated) values for \\(Q_{95}\\), \\(Q_{90}\\) and \\(Q_{80}\\) would be: lfquantile(ngaruroro$discharge, exc.freq = c(0.95, 0.9, 0.8)) ## Q95 Q90 Q80 ## 4.10925 4.93600 6.46200 The retrieved value for \\(Q_{90}\\) is 4.936 m3s-1, approximated to 4.94 m3s-1. Standardized Flow Duration Curves Comparing FDCs from different catchments requires standardization eg. dividing the discharges by the catchment area, the median or the mean discharge. lambournRay &lt;- international %&gt;% filter(river %in% c(&quot;Lambourn&quot;, &quot;Ray&quot;)) %&gt;% dplyr::select(river, area = catchment, data) lambournRay &lt;- lambournRay %&gt;% mutate( data = map2(data, area, ~mutate(.x, rel.discharge = discharge * 1000 / .y)) ) %&gt;% print() ## # A tibble: 2 x 3 ## river area data ## &lt;chr&gt; &lt;dbl&gt; &lt;list&gt; ## 1 Ray 19 &lt;tibble [20,454 × 3]&gt; ## 2 Lambourn 234 &lt;tibble [20,973 × 3]&gt; fdc &lt;- lambournRay %&gt;% dplyr::select(river, data) %&gt;% unnest(cols = data) %&gt;% group_by(river) %&gt;% mutate( freq.exc = exceedance_frequency(rel.discharge) ) ggplot(fdc, mapping = aes(x = freq.exc * 100, y = rel.discharge, size = river)) + geom_step(direction = &quot;vh&quot;) + scale_x_continuous(expand = expansion()) + scale_y_log10(breaks = breaks_log10_all(mult.base = 1), minor_breaks = breaks_log10_all(), expand = expansion()) + scale_size_manual(&quot;River&quot;, values = c(0.1, 0.75)) + labs(x = &quot;Exceedance Frequency (%)&quot;, y = expression(paste(&quot;Flow (&quot;, l, s^{-1}, km^{-2}, &quot;)&quot;))) Worked example 5.2: Mean annual minimum n-day flow Loading the Data In this example we are again going to use river flow data from the river Ngaruroro at Kuripapango (NZ) of the International Data Set in the package hydroDrought. Ten years of daily data are used as an example, as in Worked Example 5.1, to estimate mean annual minimum of the \\(n\\)-day average flow for \\(n\\) equal to 1, 7 and 30 days. For this station the lowest flows are observed around the turn of the calendar year. Therefore the annual minima are selected from years starting 1 September and ending 31 August. Table 5.4 lists the first flow values. The first two columns show the date and the corresponding flow value, \\(Q\\). In order to calculate the mean annual minimum each observation will be attributed to a year according to the date of the observation using the function water_year() which appends an additional column named year to the dataset. library(tidyverse) library(hydroDrought) # attribute each observation to the correct year # and dplyr::select only the years between 1990/91 and 2000/01 ngaruroro &lt;- international %&gt;% filter(river == &quot;Ngaruroro&quot;) %&gt;% dplyr::select(data) %&gt;% unnest(data) %&gt;% mutate( year = water_year(time, origin = &quot;-09-01&quot;) ) %&gt;% filter(year &gt;= 1990, year &lt;= 1999) smoothed &lt;- ngaruroro %&gt;% mutate( MA1 = moving_average(discharge, n = 1), MA7 = moving_average(discharge, n = 7), MA30 = moving_average(discharge, n = 30) ) ⊕Table 5.4 Calculation of \\(n\\)-day average flow (unit: in m3s-1), River Ngaruroro at Kuripapango, NZ. A moving average with a window length \\(n\\) introduces \\(n-1\\) missing values (NA values). ## # A tibble: 31 x 6 ## time discharge year MA1 MA7 MA30 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990-09-01 19.5 1990 19.5 NA NA ## 2 1990-09-02 17.9 1990 17.9 NA NA ## 3 1990-09-03 16.7 1990 16.7 NA NA ## 4 1990-09-04 17.5 1990 17.5 NA NA ## 5 1990-09-05 21.2 1990 21.2 NA NA ## 6 1990-09-06 30.8 1990 30.8 NA NA ## 7 1990-09-07 24.5 1990 24.5 21.1 NA ## 8 1990-09-08 20.7 1990 20.7 21.3 NA ## 9 1990-09-09 18.7 1990 18.7 21.4 NA ## 10 1990-09-10 16.9 1990 16.9 21.5 NA ## 11 1990-09-11 15.7 1990 15.7 21.2 NA ## 12 1990-09-12 14.6 1990 14.6 20.3 NA ## 13 1990-09-13 13.6 1990 13.6 17.8 NA ## 14 1990-09-14 12.8 1990 12.8 16.1 NA ## 15 1990-09-15 12.0 1990 12.0 14.9 NA ## 16 1990-09-16 11.3 1990 11.3 13.9 NA ## 17 1990-09-17 10.7 1990 10.7 13.0 NA ## 18 1990-09-18 10.2 1990 10.2 12.2 NA ## 19 1990-09-19 9.66 1990 9.66 11.5 NA ## 20 1990-09-20 9.22 1990 9.22 10.8 NA ## 21 1990-09-21 8.82 1990 8.82 10.3 NA ## 22 1990-09-22 8.54 1990 8.54 9.77 NA ## 23 1990-09-23 8.31 1990 8.31 9.34 NA ## 24 1990-09-24 8.14 1990 8.14 8.98 NA ## 25 1990-09-25 8.12 1990 8.12 8.69 NA ## 26 1990-09-26 8.05 1990 8.05 8.46 NA ## 27 1990-09-27 7.62 1990 7.62 8.23 NA ## 28 1990-09-28 7.42 1990 7.42 8.03 NA ## 29 1990-09-29 7.34 1990 7.34 7.86 NA ## 30 1990-09-30 8.67 1990 8.67 7.91 13.5 ## 31 1990-10-01 17.4 1990 17.4 9.24 13.4 Calculation First the annual minimum values are extracted and then the mean annual minimum values, \\(MAM(1)\\), \\(MAM(7)\\) and \\(MAM(30)\\) are calculated by averaging the annual minimum time series. The results are tabulated in Table 5.5. # compute the annual minima am &lt;- smoothed %&gt;% dplyr::select(-discharge, -time) %&gt;% group_by(year) %&gt;% summarise_all(min, na.rm = TRUE) # average the annual minima to get the mean annual minima mam &lt;- am %&gt;% dplyr::select(-year) %&gt;% summarise_all(mean) ⊕Table 5.5 \\(MAM(n-day)\\), \\(n=1\\) day, 7 days and 30 days (m3s-1). ## # A tibble: 1 x 3 ## MA1 MA7 MA30 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.13 4.39 5.43 Fast Track Mapping over the length of the smoothing window avoids the multiple explicit calls of the function mean_annual_minimum(). The results are absolutely identical but the code is can be adapted more easily and is less error-prone. # calculating each column explicitly ngaruroro %&gt;% dplyr::select(discharge, time) %&gt;% summarise( `MAM(1)` = mean_annual_minimum(discharge, time, origin = &quot;-09-01&quot;, n = 1), `MAM(7)` = mean_annual_minimum(discharge, time, origin = &quot;-09-01&quot;, n = 7), `MAM(30)` = mean_annual_minimum(discharge, time, origin = &quot;-09-01&quot;, n = 30) ) %&gt;% flatten_dbl() ## MAM(1) MAM(7) MAM(30) ## 4.130100 4.385514 5.430773 # Applying the function mean_annual_minimum() to each element of the vector c(1, 7, 30) %&gt;% map( .f = mean_annual_minimum, discharge = ngaruroro$discharge, time = ngaruroro$time, origin = &quot;-09-01&quot; ) %&gt;% flatten_dbl() ## MAM(1) MAM(7) MAM(30) ## 4.130100 4.385514 5.430773 Worked Example 5.3: Base Flow Index Loading the Data Three years of daily flow (1995 to 1997) from the Ray at Grendon Underwood (UK) have been selected. The base flow separation is done for the whole three-year period, whereas the BFI is calculated for the mid-year 1996. This ensures that days at the start and end of the calculation year are included. In Table 5.7 the calculation steps are illustrated using data from the beginning of the record. library(tidyverse) library(lubridate) library(hydroDrought) fmt_number &lt;- function(x) format(x, big.mark = &quot;&lt;i&gt;&amp;#8239;&lt;/i&gt;&quot;) ray &lt;- international %&gt;% filter(river == &quot;Ray&quot;) %&gt;% dplyr::select(data) %&gt;% unnest(data) %&gt;% mutate( year = water_year(time) ) %&gt;% filter(year &gt;= 1995, year &lt;= 1997) Calculation The daily flows, \\(Q\\) m3s-1, are divided into non-overlapping blocks of five days (Column 1 and 2, Table 5.7). ray &lt;- ray %&gt;% mutate( block = ((row_number() - 1) %/% 5) + 1 ) %&gt;% print() ## # A tibble: 1,096 x 4 ## time discharge year block ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1995-01-01 0.109 1995 1 ## 2 1995-01-02 0.063 1995 1 ## 3 1995-01-03 0.043 1995 1 ## 4 1995-01-04 0.039 1995 1 ## 5 1995-01-05 0.229 1995 1 ## 6 1995-01-06 0.186 1995 2 ## 7 1995-01-07 0.116 1995 2 ## 8 1995-01-08 0.111 1995 2 ## 9 1995-01-09 0.095 1995 2 ## 10 1995-01-10 0.123 1995 2 ## # … with 1,086 more rows Mark the minima of each of these blocks and let them be called \\(Q_{min_1}\\), … \\(Q_{min_n}\\) (Column 3, Table 5.7). Consider in turn (\\(Q_{min_1}\\), \\(Q_{min_2}\\), \\(Q_{min_3}\\)), … (\\(Q_{min_{n-1}}\\), \\(Q_{min_{n}}\\), \\(Q_{min_{n+1}}\\)). In each case, if 0.9·central value ≤ outer values, then the central value is identified as a turning point for the base flow line (bold lines in Table 5.7). Continue this procedure until the whole time series has been analysed. points &lt;- ray %&gt;% group_by(block) %&gt;% slice_min(discharge, with_ties = FALSE) %&gt;% ungroup() %&gt;% rename(Qmin = discharge) %&gt;% mutate( Qmin.red = 0.9 * Qmin, is.turning.point = Qmin.red &lt;= lag(Qmin) &amp; Qmin.red &lt;= lead(Qmin) ) Join the turning points by straight lines to form the base flow separation line and assign to each day a base flow value \\(Q_b\\), by linear interpolation between the turning points. If, on any day, the base flow estimated by this line exceeds the total flow, the base flow is set to be equal to the total flow \\(Q\\), on that day. tp &lt;- points %&gt;% filter(is.turning.point) %&gt;% dplyr::select(time, Qmin) baseflow &lt;- ray %&gt;% mutate( baseflow = approx(x = tp$time, y = tp$Qmin, xout = time)$y, baseflow = pmin(baseflow, discharge) ) Calculate the volume of water (\\(V_{base}\\)) beneath the base flow hydrograph between the first and last date of interest. The volume (m3) is simply derived as the sum of the daily base flow values multiplied by 86 400 (the number of seconds per day). Calculate the corresponding volume of water beneath the recorded hydrograph (\\(V_{total}\\)). The volume (m3) is obtained by summing the daily flow values between the first and the last dates inclusive. volume &lt;- baseflow %&gt;% filter(year == 1996) %&gt;% na.omit() %&gt;% summarise( total = sum(discharge) * 86400, baseflow = sum(baseflow) * 86400 ) The BFI is then \\(V_{base}/V_{total}\\). bfi &lt;- volume$baseflow / volume$total bfi ## [1] 0.2024139 Table 1: Table 5.7 Calculation of the base flow separation line from time series of daily flow; non-overlapping 5-day blocks are indicated by alternating background colors and turning points are marked bold. 1 . Date 2 . Daily flow 3 . Qmin 4 . 0.9 * Qmin 5 . Base flow 1995-01-01 0.109 1995-01-02 0.063 1995-01-03 0.043 1995-01-04 0.039 0.039 0.0351 1995-01-05 0.229 1995-01-06 0.186 1995-01-07 0.116 1995-01-08 0.111 1995-01-09 0.095 0.095 0.0855 1995-01-10 0.123 1995-01-11 0.178 1995-01-12 0.091 1995-01-13 0.076 1995-01-14 0.073 1995-01-15 0.062 0.062 0.0558 1995-01-16 0.054 0.054 0.0486 0.054 1995-01-17 1.060 0.056 1995-01-18 0.856 0.058 1995-01-19 1.050 0.060 1995-01-20 1.340 0.062 1995-01-21 1.640 0.064 1995-01-22 1.350 0.067 1995-01-23 0.559 0.069 1995-01-24 0.255 0.255 0.2295 0.071 1995-01-25 0.644 0.073 1995-01-26 0.793 0.075 1995-01-27 0.896 0.077 1995-01-28 0.631 0.079 1995-01-29 1.000 0.081 1995-01-30 0.492 0.492 0.4428 0.083 1995-01-31 0.377 0.085 1995-02-01 1.670 0.087 1995-02-02 0.448 0.090 1995-02-03 0.237 0.092 1995-02-04 0.163 0.163 0.1467 0.094 1995-02-05 0.123 0.096 1995-02-06 0.102 0.098 1995-02-07 0.100 0.100 0.0900 0.100 1995-02-08 0.151 0.107 1995-02-09 0.178 0.115 Results The first and second turning points are found on day 1995-01-16 and day 1995-01-16 (Column 4, Table 5.7), respectively, and a linear interpolation is used to estimate the base flow at time steps (days) between these dates (Column 5, Table 5.7). The daily base flow separation line is subsequently calculated for the whole period by linear interpolation between all turning points. The volume beneath the base flow line, \\(V_{base}\\), for 1996 is found to be 348 494.5 m3, whereas the volume of the total flow, \\(V_{total}\\), is 1 721 693 m3. The resultant BFI is 0.20. The base flow separation line for River Ray in 1996 is shown in Figure 5.4 of the book. Fast Track The base flow for a given time series can also be calculated directly using the function baseflow(), optionally with a different choice of the block length (\\(N = x\\) days) or the turning point factor or parameter (\\(TP\\)) for the central value. The default values are tp.factor = 0.9 and block.len = 5, which can be adopted to the type of flow regime studied and changed accordingly by the user. bf &lt;- ray %&gt;% dplyr::select(time, discharge) %&gt;% mutate( baseflow = baseflow(discharge, tp.factor = 0.9, block.len = 5) ) %&gt;% filter(year(time) == 1996) %&gt;% print() ## # A tibble: 366 x 3 ## time discharge baseflow ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1996-01-01 0.19 0.0332 ## 2 1996-01-02 0.224 0.0326 ## 3 1996-01-03 0.189 0.0320 ## 4 1996-01-04 0.145 0.0314 ## 5 1996-01-05 0.221 0.0308 ## 6 1996-01-06 0.385 0.0302 ## 7 1996-01-07 0.567 0.0296 ## 8 1996-01-08 0.463 0.0291 ## 9 1996-01-09 1.38 0.0285 ## 10 1996-01-10 0.475 0.0279 ## # … with 356 more rows Figure 2: Figure 5.4 Annual recorded hydrograph and calculated continuous base flow line for UK river Ray in year 1996 based on the BFI separation procedure (Worked Example 5.3). Worked example 5.4: Threshold level method The threshold level method can be used to dplyr::select drought events from time series of river flow as long as there are not too many missing values in the dataset and a meaningful threshold \\(Q_0\\) is chosen. Data from River Ngaruroro at Kuripapango (NZ) are used to demonstrate the procedure in the example below. Loading the Data 56 years of daily flow (20 September 1963 to 8 October 2019) are analysed. In this river the low flow period covers the turn of the calendar year. To avoid problems with allocating droughts to a specific calendar year because of drought events starting in one year and ending in another year, the start of the year is set to 1 September. An event is attributed to the year it starts. library(tidyverse) library(hydroDrought) ngaruroro &lt;- international %&gt;% filter(river == &quot;Ngaruroro&quot;) %&gt;% dplyr::select(data) %&gt;% unnest(data) Missing values The time series, Ngaruroro, contains missing values. We do not know if a missing value (NA) represents a flow below the threshold or above the threshold, as the flow value itself is unknown. A single missing value will cause the function drought_events() to terminate a dry spell (drought event) or similar, a wet spell. Accordingly, most characteristics derived for this event (e.g. drought duration, drought termination, drought volume, etc.) will not be correct. A conservative approach would be to eliminate years with missing values completely. Instead, to avoid losing too many years of observations, we filled periods of missing data with linear interpolation if they are of short duration. Here short duration is defined as periods &lt; 15 days, whereas years containing long periods of missing values (≥15 days) have been removed. This results in 49 years of daily flow (1 September 1964 to 31 August 2019). In total eight years are omitted from the series (1963/64, 1965/66, 1977/78, 1978/79, 1986/87, 1987/88, 2001/02 and 2019/20). ngaruroro &lt;- ngaruroro %&gt;% sanitize_ts(approx.missing = 14) %&gt;% mutate( year = water_year(time, origin = &quot;-09-01&quot;) ) coverage &lt;- ngaruroro %&gt;% filter(!is.na(discharge)) %&gt;% pull(time) %&gt;% coverage_yearly(origin = &quot;-09-01&quot;) incomplete &lt;- coverage %&gt;% filter(days.missing &gt; 0) complete &lt;- coverage %&gt;% filter(days.missing == 0) ngaruroro &lt;- ngaruroro %&gt;% anti_join(incomplete, by = &quot;year&quot;) The table below displays the year removed, the total number of days in the year (365 or 366 for leap years), the number of days with flow observations, the number of NA-values (days with missing data) and the remaining fraction of days. print(incomplete) ## # A tibble: 8 x 5 ## year days.in.year days.with.data days.missing coverage ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1963 366 347 19 0.948 ## 2 1965 365 294 71 0.805 ## 3 1977 365 350 15 0.959 ## 4 1978 365 305 60 0.836 ## 5 1986 365 341 24 0.934 ## 6 1987 366 336 30 0.918 ## 7 2001 365 344 21 0.942 ## 8 2019 366 38 328 0.104 Threshold selection and drought events A sequence of drought events is obtained from the streamflow hydrograph by considering periods with flow below a certain threshold, \\(Q_0\\). In this example \\(Q_{90} = 4.95\\)m3s-1 is used as threshold. A table of drought characteristics is derived with the function drought_events(). q90 &lt;- lfquantile(ngaruroro$discharge, exc.freq = 0.9) %&gt;% print() ## Q90 ## 4.949 droughts &lt;- ngaruroro %&gt;% drought_events(threshold = q90, pooling = &quot;none&quot;) ⊕Table 5.8 Drought deficit characteristics, River Ngaruroro at Kuripapango, NZ. ## # A tibble: 210 x 7 ## event first.day last.day duration volume qmin tqmin ## &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 1 1967-04-23 1967-04-23 1 days 6307. 4.88 1967-04-23 ## 2 2 1967-04-26 1967-04-26 1 days 4579. 4.90 1967-04-26 ## 3 3 1967-05-09 1967-05-10 2 days 17453. 4.80 1967-05-10 ## 4 4 1967-05-13 1967-05-14 2 days 18835. 4.76 1967-05-14 ## 5 5 1967-05-23 1967-05-23 1 days 3024. 4.91 1967-05-23 ## 6 6 1968-02-07 1968-02-08 2 days 34646. 4.69 1968-02-08 ## 7 7 1968-02-17 1968-03-08 21 days 1766621. 3.44 1968-03-05 ## 8 8 1968-03-11 1968-04-02 23 days 2349562. 3.23 1968-03-26 ## 9 9 1968-04-06 1968-04-09 4 days 293933. 3.76 1968-04-08 ## 10 10 1969-03-25 1969-03-30 6 days 103766. 4.68 1969-03-26 ## # … with 200 more rows The table displayed above includes: first.day: the start date, defined as the first day below the threshold; last.day: the end date, defined as the last day below the threshold; duration: the drought duration (days), defined as last.day - first.day + 1 volume: the deficit volume in m3, defined as the sum of the daily deficit flows times the duration in days; qmin: the minimum flow in m3s-1, defined as the minimum flow \\(Q_{min}\\) within a drought event; tqmin: the date of the minimum flow. Removing minor droughts (Filtering) Several minor droughts, lasting for a few days only, can be observed. To reduce the problem of minor droughts two restrictions are imposed: a minimum drought duration, \\(d_{min}\\) which removes droughts with duration less than a specified number of days; a minimum drought deficit volume (coefficient \\(\\alpha\\)), which removes droughts with a deficit volume less than a certain fraction \\(\\alpha\\) of the maximum drought deficit volume observed in the complete series of drought events. We will append a logical column called is.minor to the table of drought events. It is TRUE when drought duration is less than five days OR if the drought volume is less than 5% of the maximum drought deficit volume (i.e., 51 133.25 m3). In total 99 droughts are considered minor, and thus removed, based on these criteria. droughts &lt;- droughts %&gt;% mutate(is.minor = duration &lt; 5 | volume &lt; max(volume) * 0.005) print(droughts) ## # A tibble: 210 x 8 ## event first.day last.day duration volume qmin tqmin is.minor ## &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;lgl&gt; ## 1 1 1967-04-23 1967-04-23 1 days 6307. 4.88 1967-04-23 TRUE ## 2 2 1967-04-26 1967-04-26 1 days 4579. 4.90 1967-04-26 TRUE ## 3 3 1967-05-09 1967-05-10 2 days 17453. 4.80 1967-05-10 TRUE ## 4 4 1967-05-13 1967-05-14 2 days 18835. 4.76 1967-05-14 TRUE ## 5 5 1967-05-23 1967-05-23 1 days 3024. 4.91 1967-05-23 TRUE ## 6 6 1968-02-07 1968-02-08 2 days 34646. 4.69 1968-02-08 TRUE ## 7 7 1968-02-17 1968-03-08 21 days 1766621. 3.44 1968-03-05 FALSE ## 8 8 1968-03-11 1968-04-02 23 days 2349562. 3.23 1968-03-26 FALSE ## 9 9 1968-04-06 1968-04-09 4 days 293933. 3.76 1968-04-08 TRUE ## 10 10 1969-03-25 1969-03-30 6 days 103766. 4.68 1969-03-26 FALSE ## # … with 200 more rows Eliminating dependent droughts (Pooling) The inter-event time criterion (IC) is used to pool dependent droughts, which are droughts separated by a short period of flow above the threshold. If the time between two droughts is less than a critical duration, \\(t_{min}\\), the two events are pooled. In this example \\(t_{min}\\) is set equal to two days. pooled &lt;- ngaruroro %&gt;% drought_events( threshold = q90, pooling = &quot;inter-event&quot;, pooling.pars = list(min.duration = 2, min.vol.ratio = Inf) ) %&gt;% filter(duration &gt;= 5, volume &gt; max(volume) * 0.005) %&gt;% arrange(desc(duration)) %&gt;% print() ## # A tibble: 100 x 9 ## event first.day last.day duration dbt volume qmin tqmin pooled ## &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; &lt;drtn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; ## 1 166 2015-01-06 2015-03-15 69 days 69 days 10226650. 2.17 2015-03-04 0 ## 2 37 1974-01-20 1974-03-17 57 days 56 days 6506957. 2.88 1974-03-15 1 ## 3 138 2008-01-10 2008-03-01 52 days 51 days 6412608 2.64 2008-02-28 1 ## 4 30 1973-01-26 1973-03-12 46 days 46 days 6583939. 2.66 1973-03-03 0 ## 5 50 1983-02-16 1983-04-02 46 days 46 days 7381066. 2.46 1983-03-30 0 ## 6 148 2009-03-14 2009-04-26 44 days 43 days 6008774. 2.53 2009-04-19 1 ## 7 157 2013-02-07 2013-03-18 40 days 40 days 7089898. 2.38 2013-03-15 0 ## 8 124 2005-02-06 2005-03-16 39 days 38 days 4987094. 2.68 2005-03-13 1 ## 9 139 2008-03-09 2008-04-14 37 days 36 days 5215795. 2.65 2008-04-06 1 ## 10 59 1989-04-01 1989-04-29 29 days 29 days 2706480 3.3 1989-04-29 0 ## # … with 90 more rows When drought events are pooled the table of drought events contain two more columns: dbt: the duration below the threshold, i.e. the drought duration minus short period(s) above the threshold (note: the ‘full’ duration can be derived from the start and end date of each event); pooled: the number of drought events. The drought deficit characteristics of the ten longest (pooled) drought events are given in the table above. In total, there are 100 drought events, which equal an average of 2.04 events per year. Key drought characteristics for all drought events occurring in the period (09.1963- 08.2020), can be summarized for different drought metrics. In the example below, for each year, the number of droughts in the year, the days below the threshold (summed over all events) in a year and the minimum flow in a year, are presented: pooled %&gt;% mutate( year = water_year(first.day, origin = &quot;-09-01&quot;) ) %&gt;% group_by(year) %&gt;% summarise( n.droughts = n(), real.duration = sum(dbt), min.flow = min(qmin) ) ## # A tibble: 35 x 4 ## year n.droughts real.duration min.flow ## * &lt;dbl&gt; &lt;int&gt; &lt;drtn&gt; &lt;dbl&gt; ## 1 1967 2 44 days 3.23 ## 2 1968 2 22 days 3.88 ## 3 1969 3 37 days 3.89 ## 4 1970 3 34 days 3.73 ## 5 1972 7 114 days 2.66 ## 6 1973 2 63 days 2.88 ## 7 1974 2 10 days 4.31 ## 8 1975 2 18 days 4.09 ## 9 1977 1 25 days 3.46 ## 10 1982 2 57 days 2.46 ## # … with 25 more rows Time series of the drought duration are plotted in Figure 5.12. The longest drought durations (dbt) are found in 1972, 1973, 1982, 2007, 2008 and 2014. Figure 3: Figure 5.12 Time series of drought duration for River Ngaruroro at Kuripapango (NZ). Selection criteria: threshold level = \\(Q_{90}\\), \\(d_{min} = 5\\) days, \\(\\alpha = 0.005\\) and \\(t_{min} = 2\\) days. A histogram of the drought duration is seen in Figure 5.13, and a very skewed distribution is revealed. Short duration droughts are dominating with 43 events lasting less than 11 days. Only nine events lasted more than 30 days. p %&gt;% # replace_na(list(duration = 0)) %&gt;% ggplot(aes(duration)) + geom_histogram(binwidth = 5, boundary = 0, closed = &quot;left&quot;, size = 0.2, col = &quot;black&quot;, fill = &quot;grey90&quot;) + scale_x_continuous(limits = c(0, NA)) + scale_y_continuous(breaks = breaks_integer()) + labs(x = &quot;Drought duration (days)&quot;, y = &quot;Counts&quot;) Figure 4: Figure 5.13 Histogram of drought duration for River Ngaruroro at Kuripapango (NZ). Selection criteria: threshold level = \\(Q_{90}\\), \\(d_{min} = 5\\) days, \\(\\alpha = 0.005\\) and \\(t_{min} = 2\\) days. Worked example 5.5: Sequent Peak Algorithm Loading the Data Twelve years of daily data without missing values from River Ngaruroro at Kuripapango (NZ) are used as an example (1988 – 1999). library(tidyverse) library(hydroDrought) ngaruroro &lt;- international %&gt;% filter(river == &quot;Ngaruroro&quot;) %&gt;% dplyr::select(data) %&gt;% unnest(data) %&gt;% mutate( year = water_year(time, origin = &quot;-09-01&quot;) ) %&gt;% filter(year &gt;= 1988, year &lt;= 1999) %&gt;% print() ## # A tibble: 4,383 x 3 ## time discharge year ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1988-09-01 30.7 1988 ## 2 1988-09-02 84.8 1988 ## 3 1988-09-03 119. 1988 ## 4 1988-09-04 139. 1988 ## 5 1988-09-05 87.5 1988 ## 6 1988-09-06 74.3 1988 ## 7 1988-09-07 56.1 1988 ## 8 1988-09-08 70.2 1988 ## 9 1988-09-09 56.1 1988 ## 10 1988-09-10 43.1 1988 ## # … with 4,373 more rows Calculation Define the value of the desired yield (equals the threshold value). Here \\(Q_{90}\\) is used. Calculate the storage \\(S_t\\) according to Equation 5.5. Storage is appended as a new column to the time series tibble using the function storage() with discharge and the threshold as input values (Table 5.9). q90 &lt;- lfquantile(ngaruroro$discharge, exc.freq = 0.9) ng &lt;- ngaruroro %&gt;% mutate( storage = storage(discharge = discharge, threshold = q90) ) ⊕Table 5.9 SPA calculation of drought deficit volumes and duration for River Ngaruroro at Kuripapango (NZ) ## # A tibble: 4,383 x 4 ## time discharge year storage ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1988-09-01 30.7 1988 0 ## 2 1988-09-02 84.8 1988 0 ## 3 1988-09-03 119. 1988 0 ## 4 1988-09-04 139. 1988 0 ## 5 1988-09-05 87.5 1988 0 ## 6 1988-09-06 74.3 1988 0 ## 7 1988-09-07 56.1 1988 0 ## 8 1988-09-08 70.2 1988 0 ## 9 1988-09-09 56.1 1988 0 ## 10 1988-09-10 43.1 1988 0 ## # … with 4,373 more rows As long as the discharge is above, or equal to, the threshold, the storage is zero as only flows below the \\(Q_{90}\\) contributes to the storage. This happens the first time on 1989-03-14 and lasts only two days. ng %&gt;% filter(storage &gt; 0) ## # A tibble: 525 x 4 ## time discharge year storage ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1989-03-14 5.08 1988 0.0780 ## 2 1989-03-15 5.20 1988 0.0350 ## 3 1989-03-23 5.05 1988 0.115 ## 4 1989-03-24 4.89 1988 0.387 ## 5 1989-03-25 4.88 1988 0.669 ## 6 1989-03-26 5.09 1988 0.741 ## 7 1989-03-27 4.80 1988 1.10 ## 8 1989-03-28 4.64 1988 1.62 ## 9 1989-03-29 4.55 1988 2.24 ## 10 1989-03-30 5.11 1988 2.29 ## # … with 515 more rows Filtering for storage &gt; 0 and assigning new event numbers when the time increment in the (filtered) time series suddenly changes allows us to identify a series of uninterrupted sequences of positive \\(S_t\\). ng &lt;- ng %&gt;% filter(storage &gt; 0) %&gt;% mutate( event = group_const_change(time) ) %&gt;% print() ## # A tibble: 525 x 5 ## time discharge year storage event ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1989-03-14 5.08 1988 0.0780 1 ## 2 1989-03-15 5.20 1988 0.0350 1 ## 3 1989-03-23 5.05 1988 0.115 2 ## 4 1989-03-24 4.89 1988 0.387 2 ## 5 1989-03-25 4.88 1988 0.669 2 ## 6 1989-03-26 5.09 1988 0.741 2 ## 7 1989-03-27 4.80 1988 1.10 2 ## 8 1989-03-28 4.64 1988 1.62 2 ## 9 1989-03-29 4.55 1988 2.24 2 ## 10 1989-03-30 5.11 1988 2.29 2 ## # … with 515 more rows Selection of the drought deficit volume and duration The deficit volume is the maximum value in an uninterrupted sequence of positive \\(S_t\\), and the drought duration is the time from the beginning of the depletion period to the time of the maximum depletion. Accordingly, the duration of the first event is only one day. The date of the maximum depletion is also displayed. spa &lt;- ng %&gt;% group_by(event) %&gt;% summarise( volume = max(storage), duration = which.max(storage), time = time[which.max(storage)] ) ⊕Table 5.10 An extract of drought deficit volumes and durations for River Ngaruroro at Kuripapango (NZ), calculated by SPA ## # A tibble: 5 x 4 ## event volume duration time ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt; ## 1 1 0.0780 1 1989-03-14 ## 2 2 39.8 38 1989-04-29 ## 3 3 2.72 7 1989-05-24 ## 4 4 11.7 14 1990-03-09 ## 5 5 8.26 13 1990-04-24 Figure 5: Figure x.xx The relationship between between discharge \\(Q_t\\) and storage \\(Q_t\\) for the third drought event starting 1989-05-24. Results An extract of the drought duration and deficit volumes for the 12-year series is given in the output below. Note that the time series starts with a flow value less than the threshold (not knowing the previous flow values), thus the first event should be omitted from the analysis. Even though the SPA procedure is pooling minor and dependent droughts, the obtained time series of events still contains a number of minor drought events. Fast Track ngaruroro %&gt;% drought_events(threshold = q90, pooling = &quot;sequent-peak&quot;) ## # A tibble: 41 x 8 ## event first.day last.day duration dbt volume qmin tqmin ## &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; &lt;drtn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 1 1989-03-14 1989-03-15 1 days 2 days 0.0780 5.08 1989-03-14 ## 2 2 1989-03-23 1989-05-04 38 days 43 days 39.8 3.3 1989-04-29 ## 3 3 1989-05-18 1989-05-24 7 days 7 days 2.72 4.49 1989-05-23 ## 4 4 1990-02-24 1990-03-09 14 days 14 days 11.7 3.87 1990-03-09 ## 5 5 1990-04-12 1990-04-24 13 days 13 days 8.26 4.18 1990-04-23 ## 6 6 1990-12-27 1991-01-29 29 days 34 days 23.2 3.80 1991-01-14 ## 7 7 1991-02-11 1991-02-17 7 days 7 days 4.86 4.09 1991-02-16 ## 8 8 1991-03-30 1991-04-09 10 days 11 days 4.38 4.47 1991-04-08 ## 9 9 1991-12-19 1991-12-23 5 days 5 days 0.918 4.77 1991-12-22 ## 10 10 1991-12-28 1991-12-29 1 days 2 days 0.0590 5.10 1991-12-28 ## # … with 31 more rows Worked example 5.6, Example of how to estimate SGI using data from Stonor Park, UK In order to compare features of groundwater droughts using groundwater level data from different boreholes, Bloomfield and Marchant (2013) introduced the Standardised Groundwater level Index (SGI). The SGI uses the normal scores transform (Everitt, 2002), a nonparametric normalisation method which assigns a value to ranked observation of groundwater levels for a given month from a given hydrograph. A non-parametric approach to standardisation was favoured by Bloomfield and Marchant (2013) as they showed that no consistent parametric models could be fitted to a wide range of groundwater hydrographs, and that even when a hydrograph for a single site is considered no consistent parametric model could be fitted for all months of the year. Unlike SPI, SGI is based on a continuous variable and requires no accumulation period, however, Bloomfield and Marchant (2013) defined an SPI accumulation period (\\(q\\), in months) that gave a maximum correlation between SPI and SGI for a given site. There is no commonly agreed definition of groundwater drought status based on SGI. However, recently Bloomfield et al. (2019) defined any month with an SGI of −1 or less as being a groundwater drought month and periods of continuously negative SGI reach a monthly intensity of −1 or less was defined as an episode of groundwater by analogy with the World Meteorological Organisation definition of an SPI drought (WMO, 2012). Load the data Here we illustrate how to estimate SGI from a groundwater level time series using data from a well at Stonor Park, UK, previously described in Chapter 3. It is recommended that the standardisation is applied to data from a period of at least 30 years and that when comparing SGI from more than one site that standardisation is undertaken over a common time period. In this case, groundwater level data for Stonor Park is available for a 40 year period. library(tidyverse) library(lubridate) library(hydroDrought) stonor ## # A tibble: 2,024 x 2 ## time level ## &lt;date&gt; &lt;dbl&gt; ## 1 1969-12-28 75.0 ## 2 1970-01-04 74.6 ## 3 1970-01-11 74.2 ## 4 1970-01-18 73.9 ## 5 1970-01-25 73.6 ## 6 1970-02-01 73.5 ## 7 1970-02-08 73.7 ## 8 1970-02-15 74.0 ## 9 1970-02-22 74.2 ## 10 1970-03-01 74.5 ## # … with 2,014 more rows Create a regular times series of monthly data Step 0: The estimation of SGI requires data to be on a regular time step, in this case we will be using monthly data. The level data (recorded as metres above sea level, m aSL) from Stonor Park is already approximately on a monthly basis so we have linearly interpolated the levels to the first day of each month. Use your interpolation method of choice, or if you have more frequent observations, such as those produced by data logging systems, to sub-set the data onto a monthly time step. times &lt;- seq(as.Date(&quot;1970-01-01&quot;), as.Date(&quot;2009-12-01&quot;), by = &quot;1 month&quot;) stonor.monthly &lt;- approx(x = stonor$time, y = stonor$level, xout = times) %&gt;% as_tibble() %&gt;% rename(time = x, level = y) %&gt;% mutate( month = month(time, label = TRUE, abbr = FALSE) ) stonor.monthly ## # A tibble: 480 x 3 ## time level month ## &lt;date&gt; &lt;dbl&gt; &lt;ord&gt; ## 1 1970-01-01 74.8 January ## 2 1970-02-01 73.5 February ## 3 1970-03-01 74.5 March ## 4 1970-04-01 76.1 April ## 5 1970-05-01 77.1 May ## 6 1970-06-01 77.6 June ## 7 1970-07-01 77.3 July ## 8 1970-08-01 76.1 August ## 9 1970-09-01 74.4 September ## 10 1970-10-01 72.8 October ## # … with 470 more rows Calculate an SGI values Step 1: Extract the level data for an individual month from the full groundwater level time series. For example, in the spreadsheet example we have extracted the groundwater levels for each January in the Stonor Park record. Step 2: Order the level data for a given month from lowest to highest and estimate the standardised rank for each level, i.e. rank/number of observations in a given month + 1. Step 3: Estimate the inverse standardised normal cumulative value (mean 1, s.d. 0) from the standardise rank for each level. This value is the SGI value. In Microsoft Excel this value is returned by the =NORM.S.INV(cell) function, in R it is returned by the qnorm() function. x &lt;- stonor.monthly %&gt;% group_by(month) %&gt;% mutate( rank = rank(level), standardised.rank = rank / (n() + 1), sgi = qnorm(standardised.rank) ) x ## # A tibble: 480 x 6 ## # Groups: month [12] ## time level month rank standardised.rank sgi ## &lt;date&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1970-01-01 74.8 January 26 0.634 0.343 ## 2 1970-02-01 73.5 February 19 0.463 -0.0918 ## 3 1970-03-01 74.5 March 16 0.390 -0.279 ## 4 1970-04-01 76.1 April 14 0.341 -0.408 ## 5 1970-05-01 77.1 May 16 0.390 -0.279 ## 6 1970-06-01 77.6 June 18 0.439 -0.153 ## 7 1970-07-01 77.3 July 19 0.463 -0.0918 ## 8 1970-08-01 76.1 August 18 0.439 -0.153 ## 9 1970-09-01 74.4 September 16 0.390 -0.279 ## 10 1970-10-01 72.8 October 15 0.366 -0.343 ## # … with 470 more rows Step 4: Repeat steps 1 to 3 for data for each calendar month separately. You will end up with 12 sets of monthly level data with associated inverse standardised normal cumulative values, or SGI values. x %&gt;% nest() ## # A tibble: 12 x 2 ## # Groups: month [12] ## month data ## &lt;ord&gt; &lt;list&gt; ## 1 January &lt;tibble [40 × 5]&gt; ## 2 February &lt;tibble [40 × 5]&gt; ## 3 March &lt;tibble [40 × 5]&gt; ## 4 April &lt;tibble [40 × 5]&gt; ## 5 May &lt;tibble [40 × 5]&gt; ## 6 June &lt;tibble [40 × 5]&gt; ## 7 July &lt;tibble [40 × 5]&gt; ## 8 August &lt;tibble [40 × 5]&gt; ## 9 September &lt;tibble [40 × 5]&gt; ## 10 October &lt;tibble [40 × 5]&gt; ## 11 November &lt;tibble [40 × 5]&gt; ## 12 December &lt;tibble [40 × 5]&gt; Step 5: Combine SGI values with associated dates estimated in steps 3 and 4 and re-order oldest to most recent. Fast-Track stonor.monthly %&gt;% group_by(month) %&gt;% mutate(sgi = sgi(level)) ## # A tibble: 480 x 4 ## # Groups: month [12] ## time level month sgi ## &lt;date&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 1970-01-01 74.8 January 0.343 ## 2 1970-02-01 73.5 February -0.0918 ## 3 1970-03-01 74.5 March -0.279 ## 4 1970-04-01 76.1 April -0.408 ## 5 1970-05-01 77.1 May -0.279 ## 6 1970-06-01 77.6 June -0.153 ## 7 1970-07-01 77.3 July -0.0918 ## 8 1970-08-01 76.1 August -0.153 ## 9 1970-09-01 74.4 September -0.279 ## 10 1970-10-01 72.8 October -0.343 ## # … with 470 more rows Worked example 5.7: Rank and correlation coefficients Loading the Data library(tidyverse) library(hydroDrought) r &lt;- regional %&gt;% dplyr::select(id, river, station, data = discharge) %&gt;% print() ## # A tibble: 29 x 4 ## id river station data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 210039 Rabnitz Piringsdorf &lt;tibble [16,436 × 2]&gt; ## 2 210054 Rabnitz Mannersdorf &lt;tibble [18,262 × 2]&gt; ## 3 210062 Stoob Oberpullendorf &lt;tibble [18,262 × 2]&gt; ## 4 210088 Wulka Wulkaprodersdorf &lt;tibble [18,262 × 2]&gt; ## 5 210211 Lafnitz Dobersdorf &lt;tibble [23,741 × 2]&gt; ## 6 210237 Pinka Woppendorf &lt;tibble [23,741 × 2]&gt; ## 7 210245 Tauchenbach Altschlaining &lt;tibble [18,262 × 2]&gt; ## 8 210252 Tauchenbach Hannersdorf &lt;tibble [23,741 × 2]&gt; ## 9 210286 Strem Güssing &lt;tibble [14,610 × 2]&gt; ## 10 210294 Strem Heiligenbrunn &lt;tibble [23,741 × 2]&gt; ## # … with 19 more rows Indices # list of functions we applied to each station f &lt;- list( mean = function(x, ...) mean(x), Q50 = function(x, ...) lfquantile(x, exc.freq = 0.5), `MAM(1)` = function(x, t) mean_annual_minimum(discharge = x, time = t, n = 1), `MAM(10)` = function(x, t) mean_annual_minimum(discharge = x, time = t, n = 10), `MAM(30)` = function(x, t) mean_annual_minimum(discharge = x, time = t, n = 30), Q95 = function(x, ...) lfquantile(x, exc.freq = 0.95), Q90 = function(x, ...) lfquantile(x, exc.freq = 0.9), Q70 = function(x, ...) lfquantile(x, exc.freq = 0.7), ALPHA = function(x, t, ...) recession(time = t, discharge = x) ) indices &lt;- r %&gt;% transmute( id, indices = map(data, ~map_df(f, exec, x = .x$discharge, t = .x$time)) ) %&gt;% unnest(indices) # derived indices indices &lt;- indices %&gt;% mutate( `Q90/Q50` = Q90/Q50, `Q95/Q50` = Q95/Q50, `MAM(30)/Q50` = `MAM(30)`/Q50, `MAM(10)/Q50` = `MAM(10)`/Q50, `MAM(1)/Q50` = `MAM(1)`/Q50, ) ⊕Table 5.11 Flow indices for a subset of the Regional Data Set. ## # A tibble: 29 x 15 ## id mean Q50 `MAM(1)` `MAM(10)` `MAM(30)` Q95 Q90 Q70 ALPHA `Q90/Q50` `Q95/Q50` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2100… 0.615 0.443 0.144 0.179 0.220 0.132 0.178 0.31 0.248 0.402 0.298 ## 2 2100… 0.913 0.68 0.274 0.320 0.373 0.257 0.311 0.49 0.545 0.457 0.378 ## 3 2100… 0.613 0.429 0.129 0.164 0.216 0.14 0.179 0.305 0.472 0.417 0.326 ## 4 2100… 0.548 0.414 0.197 0.227 0.262 0.15 0.191 0.299 0.419 0.461 0.362 ## 5 2102… 6.47 4.91 2.43 2.75 3.08 2.44 2.83 3.86 0.261 0.576 0.497 ## 6 2102… 2.30 1.59 0.596 0.804 0.939 0.66 0.8 1.2 0.388 0.503 0.415 ## 7 2102… 0.409 0.348 0.166 0.183 0.201 0.124 0.159 0.255 0.196 0.457 0.356 ## 8 2102… 0.692 0.52 0.209 0.246 0.293 0.18 0.236 0.39 0.401 0.454 0.346 ## 9 2102… 1.08 0.513 0.168 0.199 0.263 0.167 0.216 0.368 0.748 0.421 0.326 ## 10 2102… 1.47 0.67 0.211 0.262 0.334 0.214 0.271 0.47 0.597 0.404 0.319 ## # … with 19 more rows, and 3 more variables: `MAM(30)/Q50` &lt;dbl&gt;, `MAM(10)/Q50` &lt;dbl&gt;, ## # `MAM(1)/Q50` &lt;dbl&gt; Ranks long &lt;- indices %&gt;% pivot_longer(cols = -id, names_to = &quot;index&quot;) %&gt;% mutate(index = factor(index, levels = setdiff(colnames(indices), &quot;id&quot;))) ranks &lt;- long %&gt;% group_by(id) %&gt;% mutate( rank = rank(value, ties.method = &quot;min&quot;) ) ggplot(ranks, aes(x = index, y = id, fill = rank, label = rank)) + geom_tile() + geom_text(size = 3) + scale_fill_viridis_c(alpha = 0.3) + labs(y = &quot;Station ID&quot;) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) Pearson correlation library(corrplot) x &lt;- indices %&gt;% dplyr::select(-id) M &lt;- cor(x, method = &quot;pearson&quot;) res1 &lt;- cor.mtest(x, method = &quot;pearson&quot;, conf.level = .95) col2 &lt;- colorRampPalette(c(&quot;#67001F&quot;, &quot;#B2182B&quot;, &quot;#D6604D&quot;, &quot;#F4A582&quot;, &quot;#FDDBC7&quot;, &quot;#FFFFFF&quot;, &quot;#D1E5F0&quot;, &quot;#92C5DE&quot;, &quot;#4393C3&quot;, &quot;#2166AC&quot;, &quot;#053061&quot;)) corrplot(M, type = &quot;upper&quot;, col = tail(head(col2(200), -30), -30), tl.cex = 0.8, addCoef.col = &quot;grey10&quot;, p.mat = res1$p, insig = &quot;pch&quot;, order = &quot;hclust&quot;, addrect = 3, rect.col = &quot;navy&quot;, pch.cex = 2, number.cex = .7, tl.col = &quot;black&quot;) Spearman (rank) correlation M &lt;- cor(x, method = &quot;spearman&quot;) res1 &lt;- cor.mtest(x, method = &quot;spearman&quot;, conf.level = .95) corrplot(M, type = &quot;upper&quot;, col = tail(head(col2(200), -30), -30), tl.cex = 0.8, addCoef.col = &quot;grey10&quot;, p.mat = res1$p, insig = &quot;pch&quot;, order = &quot;hclust&quot;, addrect = 3, rect.col = &quot;navy&quot;, pch.cex = 2, number.cex = .7, tl.col = &quot;black&quot;) Worked example 5.8: No-flow indices Loading the Data library(hydroDrought) library(tidyverse) rivers &lt;- international %&gt;% dplyr::select(river, data) Finding intermittent rivers Streamflow is on at least 5 days below the threshold of 0.001m3s-1. Removing incomplete first and last years. intermittent &lt;- rivers %&gt;% mutate( is.intermittent = map_lgl(data, ~is_intermittent(.x$time, .x$discharge)) ) %&gt;% filter(is.intermittent) %&gt;% mutate( clipped = map(data, remove_incomplete_first_last), ) Computing metrics f &lt;- list(&quot;frac nf years&quot; = no_flow_years , &quot;MAN&quot; = MAN, &quot;CVAN&quot; = CVAN, &quot;no flow days&quot; = FAN, &quot;MAMD&quot; = MAMD, &quot;onset&quot; = tau0, &quot;sd onset&quot; = tau0r, &quot;term.&quot; = tauE) metrics &lt;- intermittent %&gt;% transmute( river, metrics = map(clipped, ~map(f, exec, time = .x$time, flow = .x$discharge)) ) %&gt;% unnest_wider(metrics) %&gt;% print() ## # A tibble: 6 x 9 ## river `frac nf years` MAN CVAN `no flow days` MAMD onset `sd onset` term. ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;date&gt; ## 1 Sabar 1 185. 0.239 &lt;int [29]&gt; 168. -05-12 60.0 -11-29 ## 2 Upper Guadiana 0.116 8.12 3.26 &lt;int [40]&gt; 8.12 -08-12 25.8 -10-14 ## 3 Dawib 1 361. 0.00830 &lt;int [6]&gt; 298. -01-01 0 -01-27 ## 4 Elands 0.214 14.4 1.83 &lt;int [9]&gt; 7.89 -08-22 83.6 -09-04 ## 5 Ray 0.945 98.2 0.576 &lt;int [46]&gt; 40.2 -05-14 52.8 -05-20 ## 6 Arroyo Seco 0.441 39.0 1.33 &lt;int [118]&gt; 38.3 -07-30 24.7 -10-24 metrics %&gt;% dplyr::select(river, n.days = `no flow days`) %&gt;% unnest(n.days) %&gt;% ggplot(aes(n.days)) + geom_histogram(binwidth = 31, boundary = 0, fill = &quot;grey90&quot;, col = &quot;black&quot;, size = 0.2) + facet_wrap(vars(river)) + scale_y_continuous(breaks = breaks_integer()) + scale_x_continuous(expand = expansion(add = 7)) + labs(x = &quot;Number of no flow days per year&quot;, y = &quot;Count&quot;) + theme_bw() + theme(panel.grid.minor.y = element_blank()) Visualising streamflow permanence (spells) "],["chapter-6.html", "Chapter 6 Worked Example 6.1: Low flow frequency analysis Worked Example 6.2: Drought deficit frequency analysis", " Chapter 6 Worked Example 6.1: Low flow frequency analysis Loading the Data River Ngaruroro at Kuripapango in New Zealand (Table 4.3), has been selected for frequency analysis of annual minimum 1-day values, \\(AM(1)\\), using the Weibull (WEI) distribution. In mid-latitudes in the Northern Hemisphere, the calendar year is often used to dplyr::select the annual minimum flows. This is a suitable period for the selection of independent events as the drought or low flow period commonly occurs during the summer months. In the Southern Hemisphere the low flow season occurs at the opposite time of the year, and for Ngaruroro the lowest flows are typically found in the period November to May. As a result \\(AM(1)\\) flows were selected for a hydrological year starting at 1 September. library(tidyverse) library(hydroDrought) ngaruroro &lt;- international %&gt;% filter(river == &quot;Ngaruroro&quot;) %&gt;% dplyr::select(data) %&gt;% unnest(data) %&gt;% # linear interpolation of periods of missing values # with a duration of up to 15 days sanitize_ts(approx.missing = 15) %&gt;% mutate( year = water_year(time, origin = &quot;-09-01&quot;) ) %&gt;% print() ## # A tibble: 20,473 x 3 ## time discharge year ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1963-09-20 30.5 1963 ## 2 1963-09-21 52.8 1963 ## 3 1963-09-22 43.6 1963 ## 4 1963-09-23 37.3 1963 ## 5 1963-09-24 32.3 1963 ## 6 1963-09-25 29.0 1963 ## 7 1963-09-26 25.3 1963 ## 8 1963-09-27 22.4 1963 ## 9 1963-09-28 19.9 1963 ## 10 1963-09-29 18.2 1963 ## # … with 20,463 more rows # remove incomplete years incomplete &lt;- ngaruroro %&gt;% filter(!is.na(discharge)) %&gt;% pull(time) %&gt;% coverage_yearly(origin = &quot;-09-01&quot;) %&gt;% filter(days.missing &gt; 0) %&gt;% print() ## # A tibble: 7 x 5 ## year days.in.year days.with.data days.missing coverage ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1963 366 347 19 0.948 ## 2 1965 365 294 71 0.805 ## 3 1978 365 305 60 0.836 ## 4 1986 365 341 24 0.934 ## 5 1987 366 336 30 0.918 ## 6 2001 365 344 21 0.942 ## 7 2019 366 38 328 0.104 ngaruroro &lt;- ngaruroro %&gt;% anti_join(incomplete, by = &quot;year&quot;) # derivation of the annual minima am &lt;- annual_minima( discharge = ngaruroro$discharge, time = ngaruroro$time, origin = &quot;-09-01&quot; ) %&gt;% rename(flow = am) %&gt;% arrange(flow) %&gt;% print() ## # A tibble: 50 x 2 ## year flow ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2014 2.17 ## 2 2012 2.34 ## 3 1982 2.46 ## 4 2008 2.53 ## 5 1977 2.59 ## 6 2007 2.64 ## 7 1972 2.66 ## 8 2004 2.68 ## 9 1973 2.88 ## 10 2003 2.99 ## # … with 40 more rows Figure 6: Figure 6.9 Time series of annual minimum 1-day flow, AM(1), for River Ngaruroro at Kuripapango (NZ). Ngaruroro has a flashy river regime, but has no observed zero flows. The flows are considered to come from the same population as only a minor part of the catchment is influenced by snow in winter. The observations (daily flow series) cover the period 1 September 1964 to 31 August 2019. With the omission of 7 years (1963, 1965, 1978, 1986, 1987, 2001 and 2019) with missing data, a total of 50 annual minimum values results. The procedure used for interpolation of missing data is the same as in WE5.4. A histogram of the values is shown in Figure 6.3 (upper left). To test the assumption of stationarity, the values are plotted against time in Figure 6.9. No trend can be detected in the series and the data are therefore assumed to fulfil the requirement of independent and identically distributed data (iid). Derivation of an empirical distribution function The \\(x\\) values, AM(1), are sorted in ascending order and the rank of each value is calculated. The smallest value equals 2.175 in m3s-1 and is given rank 1. The non-exceedance probability, \\(F(x) = p\\), ('prob.emp' in the tibble below) is calculated for each \\(x\\) value using the Weibull plotting position formula \\(p_i = i / (n + 1)\\) (Equation 6.6). A probability plot for the \\(AM(1)\\) values is obtained by plotting the flow values against \\(F(x)\\) as demonstrated in Figure 6.10 (upper). A staircase pattern of several nearly equal values is observed. (Note that the \\(AM(1)\\) values now are plotted on the \\(y\\)-axis as compared to Figure 6.3, lower). The return period ('rp.emp') of the smallest event can be calculated following Equation 6.5. The non-exceedance frequency, \\(F(x)\\), of the smallest event equals 0.02 according to step 2(b), which gives a return period of 51 years. empirical &lt;- am %&gt;% mutate( rank = rank(flow), prob.emp = rank / (n() + 1), rp.emp = 1 / prob.emp ) %&gt;% arrange(rank) print(empirical) ## # A tibble: 50 x 5 ## year flow rank prob.emp rp.emp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2014 2.17 1 0.0196 51 ## 2 2012 2.34 2 0.0392 25.5 ## 3 1982 2.46 3 0.0588 17 ## 4 2008 2.53 4 0.0784 12.8 ## 5 1977 2.59 5 0.0980 10.2 ## 6 2007 2.64 6 0.118 8.5 ## 7 1972 2.66 7 0.137 7.29 ## 8 2004 2.68 8 0.157 6.38 ## 9 1973 2.88 9 0.176 5.67 ## 10 2003 2.99 10 0.196 5.1 ## # … with 40 more rows Fitting a distribution Fitting the two-parameter Weibull (WEI) distribution function (Section A6.1.4) (assuming the location parameter \\(\\xi\\) is zero) using the method of L-moments. 1 The first two L-moments (\\(\\hat\\lambda_1\\) and \\(\\hat\\lambda_2\\)) are estimated based on time series of the annual minima am$flow. These so called sample L-moments are calculated with the function samlmu() from the package lmom. library(lmom) moments &lt;- samlmu(am$flow) moments[1:2] ## l_1 l_2 ## 3.8930400 0.5560016 The parameter estimates of the WEI distribution (scale parameter \\(\\alpha\\), shape parameter \\(\\kappa\\)) are obtained following Equation A6.144: \\[\\hat\\alpha = \\frac{\\hat\\lambda_1}{\\Gamma\\left( 1 + \\frac{1}{\\hat\\kappa }\\right)}\\] \\[\\hat\\kappa = \\frac{-\\ln(2)}{\\ln\\left( 1 - \\frac{\\hat\\lambda_2}{\\hat\\lambda_1}\\right)}\\] kappa &lt;- -log(2) / log(1 - unname(moments[&quot;l_2&quot;] / moments[&quot;l_1&quot;])) alpha &lt;- unname(moments[&quot;l_1&quot;]) / gamma(1 + 1/kappa) parameter &lt;- c(location = 0, scale = alpha, shape = kappa) parameter ## location scale shape ## 0.000000 4.266122 4.497841 \\(F(x)\\) for the WEI distribution is obtained following Equation A6.1.33 (recalling that the location parameter \\(\\xi\\) is set to zero): \\[ F(x) = 1 - \\exp\\left[-\\left(\\frac{x}{\\alpha}\\right)^\\kappa\\right]\\] fitted &lt;- empirical %&gt;% dplyr::select(year, flow, prob.emp) %&gt;% mutate( prob.wei = 1 - exp(-(flow / alpha)^kappa), rp.wei = 1 / prob.wei ) The data can be plotted in a probability plot and compared to the empirical quantiles in step 2(b) (Figure 6.10, upper). The plot shows that the WEI distribution is well adjusted to the low flow extreme values, whereas deviation in the upper range may suggest that the three highest values do not belong to the low flow population (Section 6.3.3). In Figure 6.10 (lower) the empirical quantiles are plotted against the estimated distribution quantiles (QQ-plot). The points should be close to the unit diagonal if the data fit the WEI distribution well. The use of a plotting position implies that the ordered sample is plotted in regularly spaced positions. As demonstrated in the figure, the observed jumps in \\(AM(1)\\) values are reflected in the estimated Weibull quantiles, but not in the empirical quantiles. Figure 7: Figure 6.10 (upper): Estimated quantiles for annual minimum 1-day flow, AM(1), for River Ngaruroro at Kuripapango (NZ); probability plot showing the fit for the Weibull distribution (curve) to the sample data. Figure 8: Figure 6.10 (lower): QQ-plot of empirical quantiles versus Weibull quantiles Estimation of T-year events The non-exceedance frequency of the smallest value equals 0.0472 (Equation A6.1.33), which gives a return period of 21.2 years (Equation 6.5). This is 29.8 years less that the empirical estimate derived in step 2(d). fitted ## # A tibble: 50 x 5 ## year flow prob.emp prob.wei rp.wei ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2014 2.17 0.0196 0.0472 21.2 ## 2 2012 2.34 0.0392 0.0644 15.5 ## 3 1982 2.46 0.0588 0.0806 12.4 ## 4 2008 2.53 0.0784 0.0913 11.0 ## 5 1977 2.59 0.0980 0.101 9.91 ## 6 2007 2.64 0.118 0.109 9.20 ## 7 1972 2.66 0.137 0.113 8.84 ## 8 2004 2.68 0.157 0.115 8.67 ## 9 1973 2.88 0.176 0.158 6.34 ## 10 2003 2.99 0.196 0.184 5.45 ## # … with 40 more rows The 50 and 100-year events can be estimated from Equation A6.1.34: \\[ x_p = \\xi + \\alpha \\left[-\\ln( 1 - p ) \\right]^{1/\\kappa} \\] which gives: \\(\\hat x_{50} = 1.79\\)m3s-1 and \\(\\hat x_{100} = 1.53\\)m3s-1. quantile_weibull &lt;- function(prob, location = 0, scale, shape) { location + scale * (-log(1 - prob))^(1/shape) } return.period &lt;- c(50, 100) q &lt;- quantile_weibull(prob = 1/return.period, scale = alpha, shape = kappa) round(q, 2) ## [1] 1.79 1.53 Fast Track The function samlmu() from the package lmom computes the sample L-moments for a given sample. These sample L-moments are independent of the distribution one wants to fit. They have can be transferred into the parameters of the desired distribution; e.g. the function pelwei() will return the location, scale and shape parameters for a Weibull distribution. By default a three parameter Weibull distribution is used. To obtain L-moment estimates of the two parameter Weibull distribution (where the lower bound and location parameter \\(\\xi\\) is set to zero) set the argument bound = 0. library(lmom) lmom &lt;- samlmu(am$flow) par.wei &lt;- pelwei(lmom, bound = 0) par.wei ## zeta beta delta ## 0.000000 4.266122 4.497841 Quantiles for given probabilities can be calculated for many suitable distributions by the corresponding quantile function quawei(). quawei(f = c(0.02, 0.01), para = par.wei) %&gt;% round(digits = 2) ## [1] 1.79 1.53 Probability plot using the exponential reduced variate The two-parameter EXP (Equation A6.1.64) and the WEI distribution are compared using the exponential reduced variable (Box 6.4). fitted.fast &lt;- am %&gt;% mutate( prob.emp = rank(flow) / (n() + 1), prob.wei = cdfwei(x = flow, para = pelwei(lmom, bound = 0)), prob.exp = cdfexp(x = flow, para = pelexp(lmom)) ) %&gt;% arrange(flow) %&gt;% print() ## # A tibble: 50 x 5 ## year flow prob.emp prob.wei prob.exp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2014 2.17 0.0196 0.0472 0 ## 2 2012 2.34 0.0392 0.0644 0 ## 3 1982 2.46 0.0588 0.0806 0 ## 4 2008 2.53 0.0784 0.0913 0 ## 5 1977 2.59 0.0980 0.101 0 ## 6 2007 2.64 0.118 0.109 0 ## 7 1972 2.66 0.137 0.113 0 ## 8 2004 2.68 0.157 0.115 0 ## 9 1973 2.88 0.176 0.158 0.0876 ## 10 2003 2.99 0.196 0.184 0.173 ## # … with 40 more rows By substituting \\(y\\) into the expression for \\(F(x)\\), \\(y\\) can be expressed as \\(-\\ln(1 - F(x))\\) (Equation B6.4.6). The parameters of the EXP distribution are estimated using L-moments (Equation A6.1.72) and used to calculate \\(y\\) and subsequently, \\(F(x)\\). The non-exceedance probability \\(F(x)\\) for the observations is determined using the Weibull plotting position formula (Equation 6.6). The \\(AM(1)\\) values are plotted against the reduced variate in Figure 6.11. A reduced variate of \\(0.02\\) corresponds to a return period of \\(50.5\\) years for minimum values (Equation 6.5). The data will plot as a straight line given they follow the EXP distribution. Again it is demonstrated that the WEI distribution fits the extreme low flow values well, and also the upper range apart from the three largest values. The two-parameter EXP distribution is less suited to model the sample. Alternatively, \\(-\\ln(X)\\) could be plotted on a Gumbel probability paper, and a straight line would result provided the data fitted the WEI distribution. Figure 9: Figure 6.11: The annual minimum 1-day flow, AM(1), plotted against the reduced variate, y, of the EXP distribution for River Ngaruroro at Kuripapango (NZ); the observations (marked as points), the two-parameter EXP distribution (continous line) and the two-parameter WEI distribution (dashed line). Worked Example 6.2: Drought deficit frequency analysis Data River Ngaruroro at Kuripapango in New Zealand, applied for frequency analysis of annual minimum series in Worked Example 6.1, is here applied for frequency analysis of drought deficit characteristics. library(tidyverse) library(hydroDrought) pooled ## # A tibble: 100 x 5 ## event year first.day duration volume ## &lt;int&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 1967 1968-02-17 21 1767. ## 2 8 1967 1968-03-11 23 2350. ## 3 10 1968 1969-03-25 9 197. ## 4 11 1968 1969-04-06 13 786. ## 5 13 1969 1969-11-17 6 241. ## 6 15 1969 1970-01-25 14 578. ## 7 16 1969 1970-02-28 17 981. ## 8 17 1970 1970-12-08 7 130. ## 9 24 1970 1972-02-03 7 297. ## 10 25 1970 1972-02-13 20 1394. ## # … with 90 more rows The pooled drought events obtained in Worked Example 5.4 (Section 5.4.1) can be concieved as a partial duration series (PDS) because after removing 77 minor droughts only 100 events above and equal to the following thresholds remained in the series. threshold ## # A tibble: 2 x 2 ## metric threshold ## &lt;chr&gt; &lt;dbl&gt; ## 1 duration 5 ## 2 volume 51 Two drought characteristics are analysed in the following: drought deficit volume and real drought duration as defined in Worked Example 5.4. Drought events were selected for River Ngaruroro and details of the selection criteria are given in Worked Example 5.4. The series cover the period 1964 to 2018 and the start of the year is set to 1 September. A total of eight years were omitted from the series, 1963/64, 1965/66, 1977/78, 1978/79, 1986/87, 1987/88, 2001/02 and 2019/20, due to missing data. As a result 49 years remained in the dataset. As only events below the \\(Q_{90}\\) percentile are selected, it might happen that the flow never becomes less than the threshold in a year (non-drought year). A total of five out of the 49 years with observations did not experience a drought (34.7%). The PDS series of drought deficit volume and real duration are plotted in Figure 6.12. Less severe values are found in the second half of the observation period for both deficit volume (upper) and duration (lower). The data are still treated as one sample as the number of observations is considered insufficient for a separate analysis of two periods. It should further be noted that a similar trend towards less severe droughts is not as pronounced for the \\(AM(1)\\) values (Worked Example 6.1). This is likely a result of the high base flow contribution in the catchment (Figure 6.6). Figure 10: Figure 6.12 PDS of drought deficit volume (upper) and real duration (lower) for River Ngaruroro at Kuripapango (NZ). Derivation of distribution function in the Journal of Statistical Software there is an article describing the extremes package. Their approach is identical to Equation 6.11 in the first edition of the book. “The quantiles of the GP df are easily obtained by setting Equation 5 equal to \\(1 - p\\) and inverting. However, the quantiles of the GP df cannot be as readily interpreted as return levels because the data no longer derive from specific blocks of equal length. Instead, an estimate of the probability of exceeding the threshold, e.g., called \\(\\zeta_u\\) , is required. Then, the value \\(x_m\\) that is exceeded on average once every \\(m\\) observations (i.e., an estimated return level) is” \\[ x_m = u + \\frac{\\sigma_u}{\\xi} \\left[ (m \\zeta_u)^\\xi - 1\\right] \\] with \\(m\\) being the return period and \\(\\zeta_u\\) the overall exceedance rate (= average number of exceedances per year). So the return period is just multiplied with the exceedance rate. \\(u\\) is the location parameter of the GPA (= threshold), \\(\\sigma_u\\) is scale and \\(\\xi\\) is shape. In our case \\(\\zeta_u = \\frac{\\text{length of PDS}}{\\text{record length in years}}\\). This transformation will introduce return periods of less than a year (or negative probabilities) for values \\(P_{PDS} &lt; 1 + \\zeta_u\\). Return periods for PDS of less than a year just imply that such an event occurs on average several times a year. \\[P_{annual} = 1 - \\frac{(1 - P_{PDS})}{\\zeta_u}\\] I’ve seen PDS return periods &lt; 1 for example in Return Periods of Hydrological Events, Rojsberg 1976. I’ve also looked at his approach of relating PDS and AMS. But for return periods &gt; 10 years quantiles of the AMS and PDS are practically identical, irrespective of the exceedance rate. Following Zelenhasic &amp; Salvai (1987) an estimate of the non-exceedance probability, \\(F(x)\\), for the largest event in each time interval is in Nizowka obtained by combining the distribution for the occurrence of events and the distribution for the magnitudes of deficit volume or duration. Here a time interval of one year is chosen. Subsequently the return period in years for a given event can be calculated. The number of drought events occurring in a time period t is commonly assumed to be Poisson distributed (Equation 6.9) with parameter \\(\\lambda t\\). In Nizowka the binomial Pascal distribution is offered along with the Poisson distribution as described in ‘Background Information NIZOWKA’ (Software, CD). The distribution that best fitted deficit volume was the Pascal distribution for the number of droughts and the GP distribution for the deficits. For duration the Pascal distribution was chosen along with the Log-Normal distribution. The \\(F(x)\\) is for drought deficit volume plotted in Figure 6.13 together with the observed values plotted using a plotting position. The chosen distribution describes the data well, with the exception of some values in the upper range. The maximum value is, however, satisfactorily modelled. library(lmom) rp &lt;- c(10, 100, 200) prob.annual &lt;- 1 - c(1 / rp) annual2pds &lt;- function(prob.annual, exc.per.year) { 1 - (1 - prob.annual)/exc.per.year } pds2annual &lt;- function(prob.pds, exc.per.year) { 1 - (1 - prob.pds) * exc.per.year } observations &lt;- pooled %&gt;% dplyr::select(duration, volume) %&gt;% mutate(across(everything(), as.numeric)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;metric&quot;) %&gt;% nest(data = value) %&gt;% left_join(threshold, by = &quot;metric&quot;) fitted &lt;- observations %&gt;% mutate( rate = map2_dbl(data, data, ~(nrow(.x) / nrow(complete))), parameter = map2(data, threshold, ~pelgpa(samlmu(.x$value), bound = .y)) ) %&gt;% print() ## # A tibble: 2 x 5 ## metric data threshold rate parameter ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 duration &lt;tibble [100 × 1]&gt; 5 2.04 &lt;dbl [3]&gt; ## 2 volume &lt;tibble [100 × 1]&gt; 51 2.04 &lt;dbl [3]&gt; Calculation of the T-year event The return period of a given event is calculated following Equation 6.4. The relationship between the drought characteristics as defined in Worked Example 5.4 and \\(F(x)\\) are given by the tabulated distribution functions in Nizowka. The design value for a particular return period, i.e. the T-year event, can be obtained from the tables for known values of \\(F(x)\\). The estimated 10-, 100- and 200-year drought events are shown in Table 6.2. ## # A tibble: 3 x 6 ## prob.annual rp.annual rp.pds prob.pds duration volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9 10. 20.4 0.951 42.1 4841. ## 2 0.99 100. 204. 0.995 91.7 15995. ## 3 0.995 200. 408. 0.998 112. 22159. Manual computation Tobias: pearhaps mention that one should not forget to set the lower bound of the GPA (location parameter) equal to the threshold. (parameter &lt;- pelgpa(samlmu(pooled$duration), bound = 5)) ## xi alpha k ## 5.0000000 8.7770258 -0.2128228 prob.annual &lt;- 1 - 1/ c(10, 100, 200) prob.pds &lt;- annual2pds(prob.annual, exc.per.year = 100 / 49) # Quantiles of the GPA quagpa(f = prob.pds, para = parameter) ## [1] 42.11736 91.67007 112.00200 A direct estimation of the Weibull parameters is shown in the Fast Track section. ↩︎ "],["chapter-7.html", "Chapter 7 Worked example 7.1: Linear trend analysis Worked Example 7.2: Non Parametric Trends Worked Example 7.3: Structural Change Analysis Worked Example 7.4: Principal Component Analysis", " Chapter 7 Worked example 7.1: Linear trend analysis Loading the Data We begin by loading the Elbe and Gota flow time series and then calculating annual minimum flow for each. We are purposefully choosing to use the calendar year (beginning January 1) for this analysis rather than water year (beginning October 1 or November 1 for many northern hemisphere countries) because the water year breakpoint occurs near the seasonal minimum flow. library(tidyverse) library(hydroDrought) library(lubridate) ### Filter to the rivers and create columns for dates flow_df &lt;- international %&gt;% filter(river == &quot;Elbe&quot; | river == &quot;Gota&quot;) %&gt;% dplyr::select(river, data) %&gt;% unnest(cols = data) %&gt;% mutate( year = year(time), jdate = yday(time) ) head(flow_df, 8) ## # A tibble: 8 x 5 ## river time discharge year jdate ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gota 1850-01-01 506 1850 1 ## 2 Gota 1850-01-02 506 1850 2 ## 3 Gota 1850-01-03 502 1850 3 ## 4 Gota 1850-01-04 502 1850 4 ## 5 Gota 1850-01-05 502 1850 5 ## 6 Gota 1850-01-06 502 1850 6 ## 7 Gota 1850-01-07 496 1850 7 ## 8 Gota 1850-01-08 496 1850 8 ### Group by year and river and then calculate min and mean flow flow_annual &lt;- flow_df %&gt;% group_by(river, year) %&gt;% summarise(min_m3s = min(discharge, na.rm=TRUE), mean_m3s=mean(discharge, na.rm=TRUE), .groups = &quot;drop_last&quot;) head(flow_annual, 8) ## # A tibble: 8 x 4 ## # Groups: river [1] ## river year min_m3s mean_m3s ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Elbe 1875 230 583. ## 2 Elbe 1876 237 912. ## 3 Elbe 1877 296 695. ## 4 Elbe 1878 296 636. ## 5 Elbe 1879 327 805. ## 6 Elbe 1880 330 823. ## 7 Elbe 1881 327 918. ## 8 Elbe 1882 345 735. Linear trend for Elbe River Consider the annual minimum flow for the Neu Darchau river (Fig. 7.8). By fitting a line of form Eq 7.43, the estimated slope is -0.0408 m3 s-1 per year, with a 95% confidence interval of -0.34 to 0.26 m3 s-1 per year. Thus, it is unclear whether the true trend is positive or negative and this is reflected in a p-value of 0.79 for the t-test of the slope coefficient. Because this p-value is well above 0.05, we accept the null hypothesis (Ho) that there is no significant trend in annual minimum flow. ### Filter to only the Elbe River elbe_annual &lt;- flow_annual %&gt;% filter(river == &quot;Elbe&quot;) ### Create an OLS linear trend model elbe_lm &lt;- lm(min_m3s~year, data = elbe_annual) ### Show results summary(elbe_lm) ## ## Call: ## lm(formula = min_m3s ~ year, data = elbe_annual) ## ## Residuals: ## Min 1Q Median 3Q Max ## -144.497 -54.136 -7.038 53.504 236.888 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 350.15092 297.03574 1.179 0.24 ## year -0.04078 0.15257 -0.267 0.79 ## ## Residual standard error: 76.1 on 142 degrees of freedom ## Multiple R-squared: 0.000503, Adjusted R-squared: -0.006536 ## F-statistic: 0.07146 on 1 and 142 DF, p-value: 0.7896 ### Insert columns for trend fit and residuals elbe_annual &lt;- elbe_annual %&gt;% mutate(fitted = fitted(elbe_lm)) %&gt;% mutate(resid = resid(elbe_lm)) %&gt;% print() ## # A tibble: 144 x 6 ## # Groups: river [1] ## river year min_m3s mean_m3s fitted resid ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Elbe 1875 230 583. 274. -43.7 ## 2 Elbe 1876 237 912. 274. -36.6 ## 3 Elbe 1877 296 695. 274. 22.4 ## 4 Elbe 1878 296 636. 274. 22.4 ## 5 Elbe 1879 327 805. 274. 53.5 ## 6 Elbe 1880 330 823. 273. 56.5 ## 7 Elbe 1881 327 918. 273. 53.6 ## 8 Elbe 1882 345 735. 273. 71.6 ## 9 Elbe 1883 298 724. 273. 24.6 ## 10 Elbe 1884 292 680. 273. 18.7 ## # … with 134 more rows ### Create dataframe for confidence interval and predition interval new_data &lt;- data.frame(year = seq(1865, 2025, 1)) elbe_conf &lt;- predict(elbe_lm, newdata=new_data, interval = &quot;confidence&quot;, level = 0.95) %&gt;% as.data.frame() %&gt;% bind_cols(new_data) %&gt;% mutate(interval = &quot;Confidence&quot;, level = 0.95) head(elbe_conf, 8) ## fit lwr upr year interval level ## 1 274.0878 246.4956 301.6801 1865 Confidence 0.95 ## 2 274.0470 246.7231 301.3710 1866 Confidence 0.95 ## 3 274.0063 246.9499 301.0626 1867 Confidence 0.95 ## 4 273.9655 247.1760 300.7549 1868 Confidence 0.95 ## 5 273.9247 247.4014 300.4480 1869 Confidence 0.95 ## 6 273.8839 247.6260 300.1418 1870 Confidence 0.95 ## 7 273.8431 247.8498 299.8364 1871 Confidence 0.95 ## 8 273.8023 248.0728 299.5318 1872 Confidence 0.95 elbe_pred &lt;- predict(elbe_lm, newdata=new_data, interval = &quot;prediction&quot;, level = 0.95) %&gt;% as.data.frame() %&gt;% bind_cols(new_data) %&gt;% mutate(interval = &quot;Prediction&quot;, level = 0.95) Figure 11: Figure 7.8: Linear trend of annual minimum flow on the Elbe River at Neu Darchau. Light region is the prediction interval, while the darker region is the confidence interval, both using a 95% interval. Linear trend for Göta River If we instead consider the annual minimum flow on the Göta River, the annual trend is 1.23 m3 s-1 per year (Fig. 7.9) with a 95% confidence interval of 0.875 to 1.59 m3 s-1per year. The t-test on this coefficient produces a p-value of 4 x 10 -9, meaning it is highly unlikely that such a strong trend would have appeared randomly in observations if there were no true underlying trend. We reject the null hypothesis, and accept the alternative hypothesis, that there is a significant trend in Göta River annual minimum flows. Remember that we set up our alternative hypothesis as a two-sided test (b1 ≠ 0), so we are testing whether there is a significant trend, regardless if it is positive or negative. We should make this clear when we report our findings. Of course, we should also report that the trend we found was that Göta River minimum flows have been increasing at an estimated rate of 1.23 m3 s-1per year since 1941. ### Filter to the Gota River and years after 1940 gota_annual &lt;- flow_annual %&gt;% filter(river == &quot;Gota&quot;) %&gt;% filter(year &gt;= 1941) ### Create an OLS linear trend model gota_lm &lt;- lm(min_m3s~year, data = gota_annual) ### Show results summary(gota_lm) ## ## Call: ## lm(formula = min_m3s ~ year, data = gota_annual) ## ## Residuals: ## Min 1Q Median 3Q Max ## -60.468 -22.089 -4.605 17.556 90.773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2527.3124 331.5982 -7.622 5.82e-11 *** ## year 1.3387 0.1675 7.992 1.14e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 33.31 on 76 degrees of freedom ## Multiple R-squared: 0.4567, Adjusted R-squared: 0.4495 ## F-statistic: 63.88 on 1 and 76 DF, p-value: 1.139e-11 ### Insert columns for trend fit and residuals gota_annual &lt;- gota_annual %&gt;% mutate(fitted = fitted(gota_lm)) %&gt;% mutate(resid = resid(gota_lm)) ### Create dataframe for confidence interval and predition interval new_data &lt;- data.frame(year = seq(1935, 2025, 1)) gota_conf &lt;- predict(gota_lm, newdata=new_data, interval = &quot;confidence&quot;, level = 0.95) %&gt;% as.data.frame() %&gt;% bind_cols(new_data) %&gt;% mutate(interval = &quot;Confidence&quot;, level = 0.95) gota_pred &lt;- predict(gota_lm, newdata=new_data, interval = &quot;prediction&quot;, level = 0.95) %&gt;% as.data.frame() %&gt;% bind_cols(new_data) %&gt;% mutate(interval = &quot;Prediction&quot;, level = 0.95) plot_ribbon &lt;- gota_conf %&gt;% bind_rows(gota_pred) plot_line &lt;- new_data %&gt;% mutate(line_fit = predict(gota_lm, newdata=new_data), Data = &quot;Linear&quot;) Figure 12: Figure 7.9: Linear trend of annual minimum flow on the Gota River. Light region is the prediction interval, while the darker region is the confidence interval, both using a 95% interval. Although this is the same river and low flow statistic modelled in Section 7.2.4, this period (1941-present) occurs after the period 1807-1937 modelled in Section 7.2.4. We purposefully excluded all data after 1940 in Section 7.2.4 because of its non-stationary trend, whereas this non-stationary nature is now our primary focus. Worked Example 7.2: Non Parametric Trends Loading the Data First we must load the data. Consider the same data as Worked Example 7.1 using annual minimum flow from the Göta River beginning in 1941. library(tidyverse) library(hydroDrought) library(lubridate) #require(Hmisc) require(zyp) ### Filter to the rivers and create columns for dates flow_df &lt;- international %&gt;% filter(river == &quot;Gota&quot;) %&gt;% dplyr::select(river, data) %&gt;% unnest(cols = data) %&gt;% mutate( year = year(time), jdate = yday(time) ) %&gt;% print() ## # A tibble: 61,498 x 5 ## river time discharge year jdate ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gota 1850-01-01 506 1850 1 ## 2 Gota 1850-01-02 506 1850 2 ## 3 Gota 1850-01-03 502 1850 3 ## 4 Gota 1850-01-04 502 1850 4 ## 5 Gota 1850-01-05 502 1850 5 ## 6 Gota 1850-01-06 502 1850 6 ## 7 Gota 1850-01-07 496 1850 7 ## 8 Gota 1850-01-08 496 1850 8 ## 9 Gota 1850-01-09 496 1850 9 ## 10 Gota 1850-01-10 496 1850 10 ## # … with 61,488 more rows ### Group by year and river and then calculate min annual flow flow_annual &lt;- flow_df %&gt;% group_by(river, year) %&gt;% summarise(min_m3s = min(discharge, na.rm=TRUE), .groups = &quot;drop_last&quot;) %&gt;% print() %&gt;% ungroup() ## # A tibble: 169 x 3 ## # Groups: river [1] ## river year min_m3s ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gota 1850 471 ## 2 Gota 1851 500 ## 3 Gota 1852 565 ## 4 Gota 1853 591 ## 5 Gota 1854 347 ## 6 Gota 1855 316 ## 7 Gota 1856 366 ## 8 Gota 1857 401 ## 9 Gota 1858 316 ## 10 Gota 1859 310 ## # … with 159 more rows ### Subset to 1941 and after flow_annual_subset &lt;- flow_annual %&gt;% filter( year &gt;= 1941) ### Run the Mann-Kendall Test for the annual min flow mk_test_results &lt;- Kendall(x=flow_annual_subset$year, y=flow_annual_subset$min_m3s) summary(mk_test_results) ## Score = 1426 , Var(Score) = 53676.67 ## denominator = 2988.465 ## tau = 0.477, 2-sided pvalue =&lt; 2.22e-16 Mann-Kendall test The Mann-Kendall test produces an S value of 1426, derived by calculating the difference between 2200 positive pairs (73.3%) and 774 negative pairs (25.8%) out of 3003 possible data pairs (29 pairs had equal values). Using this test statistic and the 78 years of observations, the Mann-Kendall test produces a p-value of &lt; 1x10-15 suggesting there is a statistically significant increase in annual minimum flow during this period. This finding agrees with our linear regression trend test (Section 7.4.1), though with slightly less confidence because we gave up some statistical power for the flexibility of a non-parametric test. As discussed earlier (Section 7.4.2), the Mann-Kendall test only considers increases/decreases and does not estimate the magnitude of the underlying trend. For this, we must calculate the Theil-Sen non-parametric line. When compared to the OLS estimate (Fig. 7.10) the Theil-Sen line shows very similar results (1.02 m3 s-1 per year), but is slightly less affected by relatively high outliers during the late 2000s. ### Run the Theil-Sen slope for annual min flow sen_slope_results &lt;- zyp.sen(min_m3s ~ year, data = flow_annual_subset) sen_slope_results ## ## Call: ## NULL ## ## Coefficients: ## Intercept year ## -2202.414 1.172 confint.zyp(sen_slope_results) ## 0.025 0.975 ## Intercept -2263.3510083 -2132.799301 ## year 0.8387097 1.553571 ### Rerun OLS trend gota_lm &lt;- lm(min_m3s~year, data = flow_annual_subset) ### Combine for plotting purposes trend_lines &lt;- flow_annual_subset %&gt;% dplyr::select(year) %&gt;% ### Only dplyr::select the year mutate( ### Create two new columns with predictions ols = gota_lm$coef[1] + gota_lm$coef[2] * year, sen = sen_slope_results$coef[1] + sen_slope_results$coef[2] * year ) %&gt;% gather(model, min_m3s, -year) ### Gather these predictions into a long format for plotting ### Re-factor the model names to make it better to plot trend_lines$model &lt;- factor(trend_lines$model, c(&quot;ols&quot;, &quot;sen&quot;), labels=c(&quot;Ordinary Least Squares (OLS)&quot;, &quot;Thiel-Sen Slope&quot;)) Figure 13: Figure 7.10: Comparison of non-parametric (Theil-Sen) trend with parametric (OLS) trend for annual minimum flows on the Gota river after 1940. High leverage outlier effects If we instead expand the Göta River time series to begin in 1934, we introduce three high leverage outliers during the late 1930s. Here, the robustness of the Theil-Sen line is more apparent. ### Subset to 1934 and after flow_annual_longer &lt;- flow_annual %&gt;% filter( year &gt;= 1934) ### Run the Mann-Kendall Test for the annual min flow mk_longer &lt;- Kendall(x=flow_annual_longer$year, y=flow_annual_longer$min_m3s) summary(mk_longer) ## Score = 921 , Var(Score) = 69373 ## denominator = 3555.47 ## tau = 0.259, 2-sided pvalue =0.00047767 ### Run the Theil-Sen slope for annual min flow sen_slope_longer &lt;- zyp.sen(min_m3s ~ year, data = flow_annual_longer) sen_slope_longer ## ## Call: ## NULL ## ## Coefficients: ## Intercept year ## -1390.5720 0.7639 confint.zyp(sen_slope_longer) ## 0.025 0.975 ## Intercept -1545.8315356 -1191.203759 ## year 0.3676471 1.142857 ### Rerun OLS trend gota_lm_longer &lt;- lm(min_m3s~year, data = flow_annual_longer) summary(gota_lm_longer) ## ## Call: ## lm(formula = min_m3s ~ year, data = flow_annual_longer) ## ## Residuals: ## Min 1Q Median 3Q Max ## -90.84 -46.66 -21.78 13.05 423.93 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 489.7563 768.4614 0.637 0.526 ## year -0.1765 0.3889 -0.454 0.651 ## ## Residual standard error: 87.96 on 83 degrees of freedom ## Multiple R-squared: 0.002476, Adjusted R-squared: -0.009543 ## F-statistic: 0.206 on 1 and 83 DF, p-value: 0.6511 By including only 6 additional years, the OLS line changes from a significantly positive trend (+1.23 m3 s-1per year) to a significantly negative trend (-0.35 m3 s-1per year). The Theil-Sen line is barely affected by these outliers, instead capturing the 70-year increasing trend. This is an extreme example and we know from Section 7.2.3 that these outliers are related to a change in lake management. Data before 1940 follows a different underlying distribution, violating the assumptions of our model (Fig. 7.11). To prove this, read ahead to Section 7.4.3. ### Combine for plotting purposes trend_lines &lt;- flow_annual_longer %&gt;% dplyr::select(year) %&gt;% ### Only dplyr::select the year mutate( ### Create two new columns with predictions ols = gota_lm_longer$coef[1] + gota_lm_longer$coef[2] * year, sen = sen_slope_longer$coef[1] + sen_slope_longer$coef[2] * year ) %&gt;% gather(model, min_m3s, -year) ### Gather these predictions into a long format for plotting ### Re-factor the model names to make it better to plot trend_lines$model &lt;- factor(trend_lines$model, c(&quot;ols&quot;, &quot;sen&quot;), labels=c(&quot;Ordinary Least Squares (OLS)&quot;, &quot;Thiel-Sen Slope&quot;)) Figure 14: Figure 7.11: Comparison of non-parametric (Theil-Sen) trend with parametric (OLS) trend for annual minimum flows on the Gota river by incorrectly including data from 1934 (prior to dam regulation). Worked Example 7.3: Structural Change Analysis Visualizing Daily Flow In previous worked examples, we used a subset of Göta River discharges, processed to produce the annual minimum flow cut either to the period before 1940 or after 1940. Plotting the raw daily data from the earliest records (Fig. 7.12), we see an abrupt and obvious structural change around 1940. There is clearly a change in variance and a change in temporal autocorrelation (flow persistence) at this point in time. library(tidyverse) library(hydroDrought) library(lubridate) library(strucchange) library(Hmisc) ### Filter to the rivers and create columns for dates flow_df &lt;- international %&gt;% filter(river == &quot;Gota&quot;) %&gt;% dplyr::select(river, data) %&gt;% unnest(cols = data) %&gt;% mutate( year = year(time), jdate = yday(time) ) %&gt;% print() ## # A tibble: 61,498 x 5 ## river time discharge year jdate ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gota 1850-01-01 506 1850 1 ## 2 Gota 1850-01-02 506 1850 2 ## 3 Gota 1850-01-03 502 1850 3 ## 4 Gota 1850-01-04 502 1850 4 ## 5 Gota 1850-01-05 502 1850 5 ## 6 Gota 1850-01-06 502 1850 6 ## 7 Gota 1850-01-07 496 1850 7 ## 8 Gota 1850-01-08 496 1850 8 ## 9 Gota 1850-01-09 496 1850 9 ## 10 Gota 1850-01-10 496 1850 10 ## # … with 61,488 more rows Figure 15: Figure 7.12: Daily discharge at the Gota River station demonstrating a visually clear structural change. Testing for a change point To avoid this shift in temporal autocorrelation, let us focus on the annual minimum flow for the entire time series. We see a severe decrease in flow from 300-600 m3 s-1prior to the 1930s and 50-200 m3 s-1 after 1940 (Fig. 7.13). Note, that examples are not normally this clear. We can apply the Chow F-test to identify the change point. ### Group by year and river and then calculate min annual flow flow_annual &lt;- flow_df %&gt;% group_by(river, year) %&gt;% summarise(min_m3s = min(discharge, na.rm=TRUE), .groups = &quot;drop_last&quot;) %&gt;% print() %&gt;% ungroup() ## # A tibble: 169 x 3 ## # Groups: river [1] ## river year min_m3s ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gota 1850 471 ## 2 Gota 1851 500 ## 3 Gota 1852 565 ## 4 Gota 1853 591 ## 5 Gota 1854 347 ## 6 Gota 1855 316 ## 7 Gota 1856 366 ## 8 Gota 1857 401 ## 9 Gota 1858 316 ## 10 Gota 1859 310 ## # … with 159 more rows Figure 16: Figure 7.13: Annual minimum discharge at the Gota River station for the full period (1850-present). ### Convert the data to a regularly spaced annual time series first_year &lt;- min(flow_annual$year, na.rm=TRUE) flow_annual_ts &lt;- flow_annual %&gt;% complete(year=full_seq(year,1)) %&gt;% ### Make sure all years are included using the complete command ts(frequency=1, start=first_year) ### Run the Chow F test for a single breakpoint, assuming there is no trend in the data (ie intercept only) ### The equation therefore looks like min_m3s is a function only of an intercept (designated by a 1) f_test_int_only &lt;- Fstats(min_m3s ~ 1, data=flow_annual_ts) sctest(f_test_int_only) ## ## supF test ## ## data: f_test_int_only ## sup.F = 789.79, p-value &lt; 2.2e-16 bp_int_only &lt;- breakpoints(f_test_int_only) summary(bp_int_only) ## ## Optimal 2-segment partition: ## ## Call: ## breakpoints.Fstats(obj = f_test_int_only) ## ## Breakpoints at observation number: ## 88 ## ## Corresponding to breakdates: ## 1937 ## ## RSS: 951054.5 Fig. 7.14 shows the null model in green, assuming the data come from the same distribution, and the alternative model in blue, assuming a sudden change point. In this example, we choose to constrain the model to assume there is a constant mean with no trends, i.e. b1=b2a=b2b=0. Using a single breakpoint, we see that the F-test statistic is maximized with a breakpoint in 1937. This breakpoint is highly statistically significant, suggesting there was a detectable structural change during this year. The 95% confidence interval around the location of this breakpoint is between 1936 and 1938, also giving us strong confidence in the location of this breakpoint. ### Run for multiple potential breakpoints all_bp_int_only &lt;- breakpoints(min_m3s ~ 1, data=flow_annual_ts) summary(all_bp_int_only) ## ## Optimal (m+1)-segment partition: ## ## Call: ## breakpoints.formula(formula = min_m3s ~ 1, data = flow_annual_ts) ## ## Breakpoints at observation number: ## ## m = 1 88 ## m = 2 88 135 ## m = 3 51 88 135 ## m = 4 51 88 115 144 ## m = 5 25 51 88 115 144 ## ## Corresponding to breakdates: ## ## m = 1 1937 ## m = 2 1937 1984 ## m = 3 1900 1937 1984 ## m = 4 1900 1937 1964 1993 ## m = 5 1874 1900 1937 1964 1993 ## ## Fit: ## ## m 0 1 2 3 4 5 ## RSS 5448868 951054 908267 894278 891014 888699 ## BIC 2244 1960 1962 1970 1979 1989 ### Best solution should minimize BIC (Bayesian Information Criterion) #plot(all_bp_int_only) ### Calculate a confidence interval around this ci_int_only &lt;- confint(all_bp_int_only) ci_int_only ## ## Confidence intervals for breakpoints ## of optimal 2-segment partition: ## ## Call: ## confint.breakpointsfull(object = all_bp_int_only) ## ## Breakpoints at observation number: ## 2.5 % breakpoints 97.5 % ## 1 87 88 89 ## ## Corresponding to breakdates: ## 2.5 % breakpoints 97.5 % ## 1 1936 1937 1938 ### Assume this breakpoint and plot the null hypothesis model (no breakpoint, intercept only) vs the alternative (breakpoint, intercept only) ### Fit both models fm0 &lt;- lm(min_m3s ~ 1, data=flow_annual_ts ) fm1 &lt;- lm(min_m3s ~ breakfactor(all_bp_int_only, breaks = 1) + 1, data=flow_annual_ts ) Figure 17: Figure 7.14: Candidate models for Chow Breakpoint Test. Null model (no breakpoint) shown in green, while the alternative model (single breakpoint) is shown in blue. Location of the breakpoint shown with a dashed line and a 95% confidence interval in red. To test the possibility of additional breakpoints, we introduce more complex models with 2-5 breakpoints (Bai and Perron, 2003; Zeileis et al. 2003). However, adding these breaks did not significantly decrease the model residuals, pointing to the presence of a single breakpoint. As with all statistical findings, it is important to validate results with real-world understanding, checking whether the findings make physical sense. In this case, we can check the historical record during the period 1936–38 to try and identify what physical processes could produce this sudden decrease in annual minimum flows on the Göta River. The Lake Vänern decree was signed in 1937, which codified water management on Lake Vanern, the headwaters of Göta River, to facilitate hydropower. This more active reservoir management focused on maintaining a regular pattern of seasonal flow characterized by low summer flows (150 m3 s-1) and high winter flows (900 m3 s-1), with occasional fluctuation between these two extremes. Prior to management, lake flows tended to increase and decrease along decadal time scales with narrower variance. In this way, the structural change both decreased annual low flows and increased peak flows to maximize hydropower. Worked Example 7.4: Principal Component Analysis Simple PCA with two variables To explain what PCA is, we can first consider two low flow indices derived based on the Regional Dataset (Section 5.x): ALPHA and Q95/Q50 (Table 5.1). The values for the 21 sites are plotted in Fig. 5.16 after being standardized to have zero mean and unit standard deviation (now referred to as ALPHA* and Q95/Q50). Because ALPHA and Q95/Q50* are correlated, PC1 captures variability along a nearly 1:1 line running from the upper left to the lower right corners (Fig. 7.17a). This new axis is PC1. PC2, is added at a 90 degree angle (orthogonal), to capture the remaining variability. As we can see from Fig. 7.17a if we only have a measurement along the PC1 direction, referred to as a loading in PCA, we could reconstruct much of the ALPHA* and Q95/Q50* values. This principle is why PCA is so valuable for data reduction. ### Select only two variables indices_matrix &lt;- indices %&gt;% dplyr::select(ALPHA, `Q95/Q50`) %&gt;% as.matrix() rownames(indices_matrix) &lt;- indices$id ### Run the PCA analysis with scaling regional_pca &lt;- prcomp(indices_matrix, scale = TRUE) # Mean (center) and standard deviation (scale) from normalization #regional_pca$center #regional_pca$scale ### View the PC Loading matrix head(regional_pca$rotation) ## PC1 PC2 ## ALPHA 0.7071068 -0.7071068 ## Q95/Q50 -0.7071068 -0.7071068 ### View the PC Scores head(regional_pca$x) ## PC1 PC2 ## 210039 -0.1404138 0.9569836 ## 210054 0.5984752 -0.7416445 ## 210062 0.6078436 -0.1317603 ## 210088 0.1721435 -0.1278319 ## 210211 -1.2826950 -0.2886183 ## 210237 -0.2715253 -0.3175126 #By default, eigenvectors in R point in the negative direction. We can adjust this with a simple change. regional_pca$rotation &lt;- -regional_pca$rotation regional_pca$x &lt;- -regional_pca$x Interpreting more complex relationships can be aided by a loading plot (Fig. 7.17b), which shows that Q95/Q50* loads positively onto PC1, while ALPHA* loads negatively. So, sites like Rosenberg and Stiefern produce a high PC1 score due to high Q95/Q50* and low ALPHA, while a site like Lipsch produces a low PC1 score. PC2 captures the remaining variance not explained by PC1. For example, PC1 measures along an axis that assumes high Q95/Q50 is typically correlated with low ALPHA* and vice versa. However, Hoheneich with its simultaneously low Q95/Q50* and ALPHA* produces a PC1 near zero, but has a strongly negative PC2 score. Note the variance explained (72.14% for PC1 and 27.86% for PC2) is shown along the axes. Understanding how to read the loading figure (Fig. 7.17b) is critical for interpreting the PC scores because the loadings (and their sign) determine the meaning of the PCs. ### Plot the PC Loading p2 &lt;- autoplot(regional_pca, loadings.label=TRUE, loadings=TRUE, loadings.label.size=4, loadings.colour=&#39;red&#39;, loadings.label.vjust = -1, scale = 0) %&gt;% + geom_hline(yintercept = 0, colour = &quot;grey20&quot;, alpha = 0.5) %&gt;% + geom_vline(xintercept=0, colour = &quot;grey20&quot;, alpha = 0.5) %&gt;% + geom_text_repel(vjust=-1, label=indices$station, size = 3, colour = &quot;grey40&quot;, alpha =0.8) %&gt;% + coord_fixed(ratio=1, xlim=c(-2.5,2.5), ylim=c(-2.5,2.5)) Figure 18: Figure 7.17: Normalized variables with the PC axes (a) and the resultant PC Loading diagram (b) for these two variables. PCA with all variables This is a trivial example, using two PCs to represent two variables (ALPHA* and Q95/50*). PCA is most commonly used as a form of data reduction, to account for similarities between measured variables while focusing only on the most important components. To test this, we can instead run PCA on all flow indices from the Regional Dataset. This produces the loading diagram shown in Fig. 7.18a. Here we see that ALPHA loads positively onto PC1, whereas all other variables load negatively onto PC1. At sites where ALPHA is high, most other variables are low, and vice versa. Loading for PC2 further separates between the raw measures (e.g. mean, Q95) that load positively, and the relative measures (e.g. Q95/50) that load negatively. There are more PCs, but Fig. 7.18a only displays PC1 and PC2. ### Remove all the naming variables indices_matrix &lt;- indices %&gt;% dplyr::select(-id, -river, -station) %&gt;% # select(ALPHA, `Q90/Q50`, `Q95/Q50`, `MAM(30)/Q50`, `MAM(10)/Q50`,`MAM(1)/Q50`) %&gt;% as.matrix() rownames(indices_matrix) &lt;- indices$id ### Run the PCA analysis with scaling regional_pca &lt;- prcomp(indices_matrix, scale = TRUE) #Adjust the sign of Eigen vectors. regional_pca$rotation &lt;- -regional_pca$rotation regional_pca$x &lt;- -regional_pca$x ### Create loading plot p1 &lt;- autoplot(regional_pca, loadings.label=TRUE, loadings=TRUE, loadings.label.size=3, loadings.colour=&#39;red&#39;, loadings.label.vjust = -1, scale = 0) %&gt;% + geom_hline(yintercept = 0, colour = &quot;grey20&quot;, alpha = 0.5) %&gt;% + geom_vline(xintercept=0, colour = &quot;grey20&quot;, alpha = 0.5) %&gt;% #+ geom_text_repel(vjust=-1, label=indices$station, size = 3, colour = &quot;grey40&quot;, alpha =0.8) %&gt;% + coord_fixed(ratio=1) As a form of data reduction, usually only the most important PCs are studied. There are several ways of deciding how many PCs to include. One method is to look at the scree plot, which graphs the proportion of variance explained (also called the eigenvalues) and only retain the components above the scree plot ‘elbow’. In Fig. 7.18b this is either at PC2 or PC3. We know that we can capture 93.2% of the total variance in the regional flow indices using only two scores, PC1 and PC2, and that these two measures are theoretically independent of one another (they should not be correlated). The user can use their knowledge of the system to determine whether PC3, which only explains 5% of the total variance, has a useful meaning or can be considered noise. ### Extract the PC eigen values and variance explained pc.importance &lt;- summary (regional_pca) ### Create scree plot data scree_plot &lt;- data.frame(cbind(Component=seq(1,dim(pc.importance$importance)[2]),t(pc.importance$importance))) scree_plot$EigenVal &lt;- scree_plot$Standard.deviation^2 scree_plot[1:5,] ## Component Standard.deviation Proportion.of.Variance Cumulative.Proportion EigenVal ## PC1 1 3.3005444 0.72624 0.72624 10.89359347 ## PC2 2 1.7570323 0.20581 0.93205 3.08716252 ## PC3 3 0.8720938 0.05070 0.98275 0.76054757 ## PC4 4 0.3639048 0.00883 0.99158 0.13242668 ## PC5 5 0.2496006 0.00415 0.99574 0.06230047 ### Create scree plot using proportion of variance explained p2 &lt;- ggplot(scree_plot, aes(Component,Proportion.of.Variance*100)) %&gt;% + geom_line() %&gt;% + geom_point() %&gt;% + scale_x_continuous(name = &quot;Principal Component&quot;, breaks=seq(0,30,2)) %&gt;% + scale_y_continuous(name = &quot;Proportion of Variance (%)&quot;, breaks=seq(0,100, 10)) %&gt;% + theme_classic(10) %&gt;% + coord_fixed(ratio = 0.1, xlim=c(0.7,8)) Figure 19: Figure 7.18: PC Loading diagram for all low flow metrics (a) and the resultant scree plot (b). "],["chapter-10.html", "Chapter 10 Worked example 10.1: Quantifying effect of human influences", " Chapter 10 Worked example 10.1: Quantifying effect of human influences If we want to quantify the human influence on hydrological drought by comparing two time series, one with and one without this human influence, we want to use the threshold of the benchmark time series to calculate droughts in both the human-influenced and benchmark time series. These are the steps that we will discuss in this Worked Example: Calculate the threshold from the benchmark time series Calculate benchmark drought characteristics for the benchmark time series with the benchmark threshold Calculate human-influenced drought characteristics for the human-influenced time series with the benchmark threshold Compare drought characteristics between the benchmark &amp; human-influenced time series Loading the data As an example we here use the Upper-Guadiana dataset with the two time series: the benchmark time series and the human-influenced time series. library(tidyverse) library(lubridate) library(hydroDrought) ### Filter to a specific period guadiana_period &lt;- guadiana %&gt;% dplyr::select(time, discharge = Qsim) %&gt;% mutate( year = water_year(time) ) %&gt;% filter(year &gt;= 1960, year &lt;= 2000) %&gt;% print(guadiana_period) ## # A tibble: 14,976 x 3 ## time discharge year ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1960-01-01 0.153 1960 ## 2 1960-01-02 0.148 1960 ## 3 1960-01-03 0.144 1960 ## 4 1960-01-04 0.14 1960 ## 5 1960-01-05 0.136 1960 ## 6 1960-01-06 0.133 1960 ## 7 1960-01-07 0.13 1960 ## 8 1960-01-08 0.128 1960 ## 9 1960-01-09 0.129 1960 ## 10 1960-01-10 0.139 1960 ## # … with 14,966 more rows range(guadiana_period$time) ## [1] &quot;1960-01-01&quot; &quot;2000-12-31&quot; The benchmark time series comprises the uninfluenced, naturalized discharge \\(Q_{sim}\\). Note that benchmark time series can be calculated from a paired catchment analysis, an upstream-downstream comparison, model naturalisation, or pre-post disturbance analysis (Sect. 10.4.1 and 10.5.1) or from model scenarios (Sect. 10.4.3, 4 and 10.5.3, 4). The benchmark time series for this catchment are modelled as described in Sect. XX. benchmark &lt;- guadiana_period %&gt;% filter(year &gt;= 1981, year &lt;= 2000) %&gt;% print() ## # A tibble: 7,305 x 3 ## time discharge year ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981-01-01 0.013 1981 ## 2 1981-01-02 0.013 1981 ## 3 1981-01-03 0.013 1981 ## 4 1981-01-04 0.013 1981 ## 5 1981-01-05 0.013 1981 ## 6 1981-01-06 0.013 1981 ## 7 1981-01-07 0.013 1981 ## 8 1981-01-08 0.012 1981 ## 9 1981-01-09 0.012 1981 ## 10 1981-01-10 0.012 1981 ## # … with 7,295 more rows The human-influenced time series is basically the time series of observed discharge \\(Q_{obs}\\) from the Upper-Guadiana catchment. influenced &lt;- guadiana_period %&gt;% filter(year &gt;= 1991, year &lt;= 2000) %&gt;% print() ## # A tibble: 3,653 x 3 ## time discharge year ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1991-01-01 0.019 1991 ## 2 1991-01-02 0.019 1991 ## 3 1991-01-03 0.018 1991 ## 4 1991-01-04 0.018 1991 ## 5 1991-01-05 0.018 1991 ## 6 1991-01-06 0.017 1991 ## 7 1991-01-07 0.017 1991 ## 8 1991-01-08 0.017 1991 ## 9 1991-01-09 0.017 1991 ## 10 1991-01-10 0.017 1991 ## # … with 3,643 more rows Threshold calculation Here we are taking the benchmark time series. First we are smoothing the column discharge with a 30-day moving average. The threshold itself is calculated as a daily varying \\(Q_{80}\\) yielding a dataset with a row for each day of the year and the appropriate \\(Q_{80}\\) in the column named threshold. threshold &lt;- guadiana_period %&gt;% # applying a 30-day moving average smoother mutate(discharge = moving_average(discharge, n = 30, sides = &quot;center&quot;)) %&gt;% # computing the Q80 for each day of the year var_threshold(vary.by = &quot;day&quot;, fun = lfquantile, exc.freq = 0.80) print(threshold) ## # A tibble: 366 x 2 ## day threshold ## * &lt;date&gt; &lt;dbl&gt; ## 1 -01-01 0.0172 ## 2 -01-02 0.0184 ## 3 -01-03 0.0184 ## 4 -01-04 0.0187 ## 5 -01-05 0.0190 ## 6 -01-06 0.0194 ## 7 -01-07 0.0197 ## 8 -01-08 0.0200 ## 9 -01-09 0.0203 ## 10 -01-10 0.0206 ## # … with 356 more rows p &lt;- ggplot(data = threshold, aes(x = day, y = threshold)) + geom_step() + scale_x_date(date_breaks = &quot;1 months&quot;, date_labels = &quot;%b&quot;, expand = expansion()) + theme(axis.title.x = element_blank()) p ### Add Julian date columns to the benchmark and threshold benchmark &lt;- benchmark %&gt;% mutate(jdate = yday(time)) threshold &lt;- threshold %&gt;% mutate(jdate = yday(day), year = year(day)) ### Plot the threshold against monthly flows in log10 space p &lt;- ggplot(benchmark, aes(x=jdate)) + geom_line(aes(group=year, y=discharge), alpha = 0.2) + geom_line(data = threshold, aes(y=threshold), colour=&quot;red&quot;) + scale_x_continuous(name = &quot;Julian Date&quot;, expand = expansion()) + scale_y_log10() p plot_df &lt;- guadiana_period %&gt;% mutate(jdate = yday(time), line = &quot;thresh_basis&quot;, facet = &quot;thresh_basis&quot;) %&gt;% bind_rows(benchmark %&gt;% mutate(line = &quot;benchmark&quot;, facet = &quot;benchmark&quot;)) %&gt;% bind_rows(influenced %&gt;% mutate(jdate = yday(time), line = &quot;influenced&quot;, facet = &quot;influenced&quot;)) %&gt;% left_join(threshold %&gt;% select(jdate, threshold), by = &quot;jdate&quot;) %&gt;% mutate(line = factor(line, levels = c(&quot;thresh_basis&quot;, &quot;benchmark&quot;, &quot;influenced&quot;), labels = c(&quot;Threshold\\nBasis&quot;, &quot;Benchmark&quot;, &quot;Human\\nInfluenced&quot;))) %&gt;% mutate(facet = factor(facet, levels = c(&quot;thresh_basis&quot;, &quot;benchmark&quot;, &quot;influenced&quot;), labels = c(&quot;Threshold Basis&quot;, &quot;Benchmark&quot;, &quot;Influenced&quot;))) %&gt;% mutate(label = &quot;Threshold&quot;) p &lt;- ggplot(plot_df, aes(x=time)) %&gt;% + geom_line(aes( y=discharge, colour = line)) %&gt;% + geom_line(aes(y = threshold), colour = &quot;grey20&quot;) %&gt;% + facet_grid(facet~.) %&gt;% + scale_y_log10() p p &lt;- ggplot(plot_df, aes(x=time)) %&gt;% + geom_line(aes( y=discharge, colour = line)) %&gt;% + facet_grid(facet~.) p p &lt;- ggplot(plot_df %&gt;% filter(year &gt;=1980 &amp; facet != &quot;Threshold Basis&quot;), aes(x=jdate)) %&gt;% + geom_line(aes( y=discharge, colour = line)) %&gt;% + geom_step(aes(y = threshold, colour = label), linetype = &quot;dashed&quot;, size = 0.2) %&gt;% + facet_grid(year ~ facet, scales = &quot;free_x&quot;) %&gt;% + scale_colour_manual(values = c(&quot;black&quot;, &quot;#377eb8&quot;, &quot;red&quot;), limits = c(&quot;Benchmark&quot;, &quot;Human\\nInfluenced&quot;, &quot;Threshold&quot;)) %&gt;% # + scale_x_date(date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) %&gt;% + scale_y_log10() p p &lt;- ggplot(plot_df %&gt;% filter(year &gt;=1980 &amp; facet != &quot;Threshold Basis&quot;), aes(x=jdate)) %&gt;% + geom_line(aes( y=discharge, colour = line)) %&gt;% + geom_step(aes(y = threshold, colour = label), linetype = &quot;dashed&quot;, size = 0.2) %&gt;% + facet_grid(year ~ facet, scales = &quot;free_x&quot;) %&gt;% + scale_colour_manual(values = c(&quot;black&quot;, &quot;#377eb8&quot;, &quot;red&quot;), limits = c(&quot;Benchmark&quot;, &quot;Human\\nInfluenced&quot;, &quot;Threshold&quot;)) %&gt;% #+ scale_x_date(date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;) %&gt;% + scale_y_log10() %&gt;% + theme_bw(8) p ggplot(benchmark, aes(x=time)) %&gt;% + geom_line(aes( y=discharge)) %&gt;% + geom_line(data = benchmark %&gt;% left_join(threshold, by = &quot;jdate&quot;), aes(y=threshold), colour=&quot;red&quot;) ### Plot the threshold against monthly flows in log10 space ya &lt;- benchmark %&gt;% mutate(discharge = moving_average(discharge, n = 30, sides = &quot;center&quot;)) %&gt;% filter(year &gt;= 1960, year &lt;= 2000) p &lt;- ggplot(ya %&gt;% drop_na(), aes(x=jdate)) + geom_line(aes(group=year, y=discharge), alpha = 0.2) + geom_line(data = threshold, aes(y=threshold), colour=&quot;red&quot;) + scale_x_continuous(name = &quot;Julian Date&quot;, expand = expansion()) + scale_y_log10() p threshold &lt;- threshold %&gt;% dplyr::select(-jdate, -year) Benchmark Drought characteristics 2 # initialize empty list for events events &lt;- list(benchmark = NULL, influenced = NULL) # initialize empty list for final drought characteristics drought.char &lt;- list(benchmark = NULL, influenced = NULL) # function that computes the drought characteristics given a table of events summarize_dc &lt;- function(x) { c(&quot;mean.duration&quot; = as.double(mean(x$duration)), &quot;mean.deficit&quot; = mean(x$volume)) } Periods with discharges below the before calculated threshold are considered drought events. Consecutive drought events with an inter-event time of less than or equal to 10 days (argument min.duration = 10) get pooled into a single drought event regardless of their inter-event excess volume (argument min.vol.ratio = Inf). To get rid of minor droughts, only drought events with a duration of more than 10 days are kept. # calculate the drought events for the benchmark time series events$benchmark &lt;- benchmark %&gt;% filter(year &gt;= 1981, year &lt;= 2000) %&gt;% drought_events( threshold = threshold, pooling = &quot;inter-event&quot;, pooling.pars = list(min.duration = 10, min.vol.ratio = Inf) ) %&gt;% filter(duration &gt; 10) # calculate the drought characteristics for the benchmark time series drought.char$benchmark &lt;- summarize_dc(events$benchmark) For the Upper-Guadiana, these would be the drought events of the benchmark time series. Note that this calculation can be applied for a different time period than was used to calculate the benchmark threshold (for example, for Upper-Guadiana, we are using the period 1981-2000). Events numbers that are missing in the sequence are minor drought events that have been filtered out. print(events$benchmark) ## # A tibble: 32 x 9 ## event first.day last.day duration dbt volume qmin tqmin pooled ## &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; &lt;drtn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; ## 1 1 1981-01-01 1981-04-22 112 days 112 days 100349. 0.01 1981-02-04 0 ## 2 2 1981-06-09 1981-12-28 203 days 187 days 84024. 0 1981-11-23 4 ## 3 5 1982-05-01 1982-06-01 32 days 30 days 7235. 0.017 1982-05-25 1 ## 4 6 1982-06-25 1982-10-19 117 days 106 days 14406. 0 1982-10-07 3 ## 5 7 1983-01-31 1983-11-22 296 days 296 days 202991. 0 1983-09-20 0 ## 6 8 1983-12-03 1983-12-17 15 days 15 days 5915. 0.009 1983-12-13 0 ## 7 9 1984-02-17 1984-02-27 11 days 11 days 3787. 0.018 1984-02-22 0 ## 8 10 1984-10-22 1984-11-03 13 days 11 days 533. 0.007 1984-10-22 1 ## 9 12 1985-10-12 1985-12-25 75 days 66 days 17653. 0.004 1985-11-12 2 ## 10 13 1986-11-29 1987-01-11 44 days 44 days 22026. 0.009 1987-01-08 0 ## # … with 22 more rows Human-influenced drought characteristics We will now do the same drought calculation on the human-influenced time series, using the same period as for the benchmark drought calculation (here: 1981-2000). # calculate the drought events for the human influenced time series events$influenced &lt;- influenced %&gt;% filter(year &gt;= 1991 &amp; year &lt;= 2000) %&gt;% drought_events(threshold = threshold, pooling = &quot;inter-event&quot;, pooling.pars = list(min.duration = 10, min.vol.ratio = Inf)) %&gt;% filter(duration &gt; 10) # calculate the drought characteristics for the human influenced time series drought.char$influenced &lt;- summarize_dc(events$influenced) Comparison of drought characteristics For the Upper-Guadiana, these would be the drought characteristics: drought.char ## $benchmark ## mean.duration mean.deficit ## 90.59375 57967.00200 ## ## $influenced ## mean.duration mean.deficit ## 97.23529 77893.12376 Calculate the percentage difference between the benchmark and human-influenced drought characteristics. \\[\\Delta DC = \\frac{DCHI - DCBM}{DCBM} \\cdot 100\\] where \\(\\Delta DC\\) is the percentage change in drought characteristics (\\(DC\\)) between the human-influenced (\\(DCHI\\)) and benchmark (\\(DCBM\\)) time series. For the Upper-Guadiana, these would be the differences in drought characteristics: (drought.char$influenced - drought.char$benchmark) / drought.char$benchmark * 100 ## mean.duration mean.deficit ## 7.331128 34.374939 Tobias: I think this Worked Example could be easier to follow if we fist compute the drought events for both time series and in a separate section the drought characteristics. So section 2 could become “Drought Events”, Section 3 “Drought Characteristics”.↩︎ "],["chapter-12.html", "Chapter 12 Worked example 12.2: Drought Impact Model", " Chapter 12 Worked example 12.2: Drought Impact Model For this example, we will determine how agricultural drought impacts in Germany are related to drought severity, by defining an impact model and using it to make predictions of impact likelihood. Data We will be considering 4 drought indices as potential predictors of drought impacts: the SPI3, SPEI3, SPI12, and SPEI12. See Section 5.5 for full definitions of the Standardized Precipitation Index (SPI) and Standardized Precipitation-Evapotransporation Index (SPEI). But for our purposes, we should know that the SPI3 normalizes precipitation from the preceding 3 months, while the SPI12 normalizes precipitation over the preceding 12 months. The SPEI uses the same number convention, but instead uses climatic water balance (precipitation minus reference potential evapotranspiration). We begin by loading the agricultural drought impact data, collected annually, and the drought indices, which are all stored in the package hydroDrought. This data is loaded when the hydroDrought package is loaded. The SPI and SPEI data is provided at a monthly resolution, so we must reduce this to an annual resolution to match the impact series. We choose to filter only to July, based on our prior knowledge of agricultural practices. library(tidyverse) library(hydroDrought) ## Push these into the function library(gridExtra) library(grid) spi_de &lt;- spi_de %&gt;% filter(month == 7) ### Merge dataframes using a full join (i.e. all records are included, even if data is missing) spi_impacts &lt;- spi_de %&gt;% full_join(impacts_de, by = &quot;year&quot;) %&gt;% ### Join on year drop_na(impact) %&gt;% ### Drop rows with NA in the impact column print(spi_impacts) ### Check results ## # A tibble: 172 x 11 ## date year month index_val index index_type index_months country impact_count nuts_n impact ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; ## 1 1970-07-01 1970 7 0.364 spei… spei 12 DE 0 13 FALSE ## 2 1971-07-01 1971 7 -0.605 spei… spei 12 DE 0 13 FALSE ## 3 1972-07-01 1972 7 -0.741 spei… spei 12 DE 0 13 FALSE ## 4 1973-07-01 1973 7 -0.758 spei… spei 12 DE 0 13 FALSE ## 5 1974-07-01 1974 7 -0.295 spei… spei 12 DE 0 13 FALSE ## 6 1975-07-01 1975 7 1.25 spei… spei 12 DE 0 13 FALSE ## 7 1976-07-01 1976 7 -1.88 spei… spei 12 DE 5 13 TRUE ## 8 1977-07-01 1977 7 -0.299 spei… spei 12 DE 0 13 FALSE ## 9 1978-07-01 1978 7 0.274 spei… spei 12 DE 0 13 FALSE ## 10 1979-07-01 1979 7 0.763 spei… spei 12 DE 0 13 FALSE ## # … with 162 more rows Preliminary visual comparison As a preliminary comparison, we can plot the time series of July SPEI3 (in blue) overlaid on the occurrence of annual impacts (vertical red bars). ### Only plot SPEI 3 plot_df &lt;- spi_impacts %&gt;% filter(index == &quot;spei_3&quot;) ### Extract the impacts for separate plotting plot_impacts &lt;- plot_df %&gt;% dplyr::select(year, impact) %&gt;% filter(impact == TRUE) %&gt;% mutate(left = year - 0.5, right = year + 0.5) ### Create the time series plot p &lt;- ggplot(plot_df, aes(x=year)) %&gt;% + geom_hline(yintercept=0, color = &quot;grey70&quot;) %&gt;% ### Make a horizontal axis at y= 0 + geom_rect(data = plot_impacts, aes(xmin=left, xmax=right, ymin=-Inf, ymax=+Inf), fill=&#39;pink&#39;, alpha=0.9) %&gt;% ### Create a pink region around the impact + geom_vline(data = plot_impacts, aes(xintercept = year), linetype=&quot;dotted&quot;, color = &quot;red&quot;) %&gt;% ### Add a vertical line at the impact + geom_line( aes(y = index_val), colour=&quot;#377eb8&quot;) %&gt;% ### Draw the time series of SPI12 + theme_classic(10) %&gt;% + scale_x_continuous(name = &quot;Year&quot;, breaks = seq(1950,2100,5)) %&gt;% + scale_y_continuous(name = &quot;SPEI-3 (July)&quot;, breaks = seq(-3,3,0.5)) p Figure 20: Figure 12.10 NEED CAPTION. When SPEI3 is very negative (approximately -2) in 1975 an impact occurs, but when SPEI3 is near -1.5, we have two impact years (2003, 2006) and two non-impact years (1989, 2008). So, it appears that at an SPEI3 of -1.5, we have an approximately 50% chance of an impact. As we approach positive SPEI3, based on this data, the number of drought impacts drops to near 0 (there are no impact records in this database). We can also visualise the relationship between SPEI3 and impact by plotting each value of SPEI3 from the time series along the x-axis and stacking the proportion of impact vs. no impact months along the y-axis (Fig. 12.11). ### Create a meaningful column for impacts when plotted plot_all_df &lt;- spi_impacts %&gt;% mutate(impact_label = case_when( impact == TRUE ~ &quot;Impact Reported&quot;, TRUE ~ &quot;No Impact&quot; ) ) %&gt;% mutate(impact_label = factor(impact_label, levels=c( &quot;No Impact&quot;, &quot;Impact Reported&quot;))) ### Create a smoothed plot showing proportion in each category p &lt;- ggplot(plot_all_df, aes(x=index_val, fill = impact_label)) %&gt;% + geom_density(position = &quot;fill&quot;) %&gt;% + geom_vline(xintercept=0, color = &quot;grey30&quot;, linetype=&quot;longdash&quot;) %&gt;% ### Make a vartical axis at x= 0 + theme_classic(8) %&gt;% + scale_x_continuous(name = &quot;Drought Index in July&quot;, breaks = seq(-5,5,0.5)) %&gt;% + scale_y_continuous(name = &quot;Proportion of Years in Impact Category&quot;, labels = scales::percent_format(accuracy = 5L), expand = c(0,0)) %&gt;% + scale_fill_brewer(type = &quot;qual&quot;, palette = &quot;Set2&quot;) %&gt;% + theme(legend.position = c(0.9, 0.9), legend.title = element_blank()) %&gt;% + facet_wrap(~ index) p Figure 21: Figure 12.11 NEED CAPTION. The relationship for SPEI3 is shown in the top-right figure (Fig. 12.11b). If we approximate the boundary between impact years (orange) and non-impact years (green), it appears there is a consistent increase in impact proportion as SPEI-3 decreases (shifts from right to left). SPEI3 appears to have the most consistent relationship with impacts (Fig 12.11b), whereas the relationship with SPI3 is confounded by some drought periods (low SPI) where the ‘No Impacts’ category dominates. If we chose a wider smoothing interval, this effect might be averaged out. For the 12-month accumulated indices (Fig 12.11a, 12.11c), years with impacts appear to be scattered throughout the SPI/SPEI range, suggesting a weaker connection. The likelihood of impacts increases for both severe drought and pluvials, further supporting the hypothesis that the 12-month indices are not useful for making the distinction between impact and no impact. Logistic Regression Figure 12.11 is a visual analogy of logistic regression, which is the more rigorous statistical method we will use to model the relationship between one or more predictors and a binary (TRUE/FALSE) predictand. Please keep this visual analogy in mind as we proceed using logistic regression to develop a quantitative impact model. For more methodological details, see Section 7.3.7, Generalized Linear Models. In R, logistic regression models are calibrated using the glm() command, which can fit a range of Generalized Linear Models (GLMs). ### Create data for fitting fit_data &lt;- spi_impacts %&gt;% dplyr::select(date, year, month, index, index_val, impact) %&gt;% spread(index, index_val) ### Fit simple logistic regression using SPEI-3 as a predictor variable ### Force an intercept to be included by using + 1 spei_3_fit &lt;- glm(impact ~ spei_3 + 1, data = fit_data, family = &quot;binomial&quot;) summary(spei_3_fit) ## ## Call: ## glm(formula = impact ~ spei_3 + 1, family = &quot;binomial&quot;, data = fit_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.64740 -0.40334 -0.29607 -0.06872 2.39563 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.0242 0.8711 -3.472 0.000517 *** ## spei_3 -2.7561 0.9576 -2.878 0.004002 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 38.207 on 42 degrees of freedom ## Residual deviance: 23.362 on 41 degrees of freedom ## AIC: 27.362 ## ## Number of Fisher Scoring iterations: 6 confint(spei_3_fit) ## 2.5 % 97.5 % ## (Intercept) -5.181009 -1.634530 ## spei_3 -5.049901 -1.167493 The fitted model appears similar to our visual analogy (Figure 12.11), but now follows a formal logistic regression equation, with confidence intervals (Fig 12.12). As expected from our visual analogy, the likelihood of an impact is approximately 50% for SPEI3 of -1.1, near 100% for -2 and near 0 for positive values. ### Plot the relationship with SPEI-3 logistbarplot(log.fit = spei_3_fit, log.var = &quot;spei_3&quot;) Figure 22: Figure 12.12 NEED CAPTION. Remember from Section 7.2.3 that logistic regression converts likelihoods of a binary response into log-odds space (also called logit space). When a line fit in this space is transformed back to probability space, it produces the characteristic S-shape curve between 0 and 1. If we had used linear regression without the logit transform, we might have produced likelihoods greater than 1 or less than 0, which are not feasible. Check the supplementary code in the repository to see the full conversion from data into log-odds space and ultimately the logistic regression. Predicting drought impacts It is now possible to make predictions based on this relationship for our fitting period or for any new values of SPI or SPEI. In R this is easily done using the predict() function. ### Use the SPEI3 regression to predict the likelihood of an impact ### Keep the standard error for confidence intervals around prediction predict_ts &lt;- predict(spei_3_fit, newdata = fit_data, type = &quot;link&quot;, se = TRUE) ### Add in prediction and confidence intervals predict_df &lt;- fit_data %&gt;% mutate(predict = plogis(predict_ts$fit)) %&gt;% mutate(predict_upper = plogis(predict_ts$fit + (qnorm(0.025) * predict_ts$se.fit))) %&gt;% mutate(predict_lower = plogis(predict_ts$fit - (qnorm(0.025) * predict_ts$se.fit))) %&gt;% dplyr::select(-spi_3, -spei_12, -spi_12) %&gt;% print() ## # A tibble: 43 x 8 ## date year month impact spei_3 predict predict_upper predict_lower ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1970-07-01 1970 7 FALSE -0.211 0.0801 0.0207 0.264 ## 2 1971-07-01 1971 7 FALSE -0.171 0.0723 0.0177 0.253 ## 3 1972-07-01 1972 7 FALSE 1.05 0.00266 0.0000819 0.0801 ## 4 1973-07-01 1973 7 FALSE -0.175 0.0730 0.0179 0.254 ## 5 1974-07-01 1974 7 FALSE 1.14 0.00207 0.0000540 0.0739 ## 6 1975-07-01 1975 7 FALSE 0.0892 0.0366 0.00599 0.193 ## 7 1976-07-01 1976 7 TRUE -1.85 0.889 0.427 0.988 ## 8 1977-07-01 1977 7 FALSE -0.415 0.132 0.0443 0.335 ## 9 1978-07-01 1978 7 FALSE 0.00736 0.0455 0.00847 0.210 ## 10 1979-07-01 1979 7 FALSE -0.328 0.107 0.0323 0.301 ## # … with 33 more rows Making predictions based on the original training data, we can now see (Fig. 12.13) that during most years the likelihood of an agricultural impact remains below 10%. In years with low SPEI3, the likelihood quickly peaks. In 1976, our most severe drought in this period, the predicted likelihood is approximately 90%. Not surprisingly, an impact occurs during this period. For a predicted likelihood of 50%, there are approximately the same number of years with impacts as those without, a sign of a good fit. Also, notice that the prediction intervals are not normally distributed around the estimate. They are normally distributed in logit (log-odds) space, but have been converted into impact likelihood, ensuring that neither the estimate nor the interval produces impossible likelihoods (&lt; 0 or &gt; 1). The supplementary code shows the process testing whether how to include a second drought index as a significant predictor. ### Create a time series plot p &lt;- ggplot(predict_df, aes(x=year)) %&gt;% + geom_hline(yintercept=0.5, color = &quot;grey70&quot;, linetype=&quot;dotted&quot;) %&gt;% ### Make a horizontal axis at y= 0 + geom_rect(data = plot_impacts, aes(xmin=left, xmax=right, ymin=-Inf, ymax=+Inf), fill=&#39;pink&#39;, alpha=0.9) %&gt;% ### Create a pink region around the impact # + geom_vline(data = plot_impacts, aes(xintercept = year), linetype=&quot;dotted&quot;, color = &quot;red&quot;) %&gt;% ### Add a vertical line at the impact + geom_ribbon(aes(ymin = predict_lower, ymax = predict_upper), alpha = 0.2, fill = &quot;grey20&quot;) %&gt;% + geom_line( aes(y = predict), colour=&quot;#377eb8&quot;) %&gt;% ### Draw the time series of SPI12 + theme_classic(8) %&gt;% + scale_x_continuous(name = &quot;Year&quot;, breaks = seq(1950,2100,5)) %&gt;% + scale_y_continuous(name = &quot;Impact Likelihood&quot;, breaks = seq(0,1,0.1), labels = scales::percent_format(accuracy = 5L), expand=c(0,0)) p Figure 23: Figure 12.13 NEED CAPTION. "],["chapter-13.html", "Chapter 13 Worked example 13.1: Forecast skill", " Chapter 13 Worked example 13.1: Forecast skill Loading the Data In this example we will be calculating the skill of the Ensemble Streamflow Prediction (ESP) forecast method for the Lambourn catchment at Shaw gauging station in southern England (catchment area of 234.1 km2). The dataset consists of monthly-averaged river discharge observations in the second column and corresponding ESP forecasts initialised on 1st January each year for a one month lead time for 51 ensemble members, in columns 3 to 53. The dataset contains 50 observation-forecast pairs across the 1965 to 2014 hindcast periods as shown in column 1. The units for both observations and forecasts are m3s-1. More detail on how these data were generated can be found in Harrigan et al. (2018). First we load the dataset and define the observations and forecast variables: # Load the data library(hydroDrought) rmarkdown::paged_table(fcst_dataset, options = list(cols.print = 5)) # Subset observations vector obs &lt;- fcst_dataset$Observed_flow # Subset ensemble forecast matrix fcst_ens &lt;- fcst_dataset[, 3:53] # Plot observed and ensemble forecast boxplot(t(fcst_ens), xlab = &quot;Hindcast year&quot;, ylab = &quot;Flow (cumecs)&quot;, xaxt = &quot;n&quot;, range = 0) axis(side = 1, at = 1:50, labels = fcst_dataset$Year) lines(obs) Calculate long-term low flow threshold from observations In this example we want to evaluate the ability of the ESP forecast to predict the probability of river discharge events below the long-term Q90 low flow threshold. We need to first calculate the long-term Q90 threshold from observations using the lfquantile() 3 function: obs_Q90_threshold &lt;- lfquantile(obs, exc.freq = 0.9) obs_Q90_threshold ## Q90 ## 0.9278767 Calculate the Brier Score (BS) The forecast evaluation metric we will use is the Brier score. The Brier score is essentially the mean squared error of the probability forecasts when the observation obs1 = 1 if the event occurs, and obs2 = 0 if the event does not occur. The score averages the squared differences between pairs of forecast probabilities and the subsequent binary observations, \\[BS=\\frac{1}{n}\\sum_{k=1}^n(fcst_{k}-obs_{k})^2\\] where the index k denotes numbering of the n forecast-event pairs. In our example n = 50. Perfect forecasts will have BS = 0. The steps to calculate the BS are as follows: Create vector of binary observations obs with 1 when an an event is observed (i.e. below the long-term observed Q90 threshold for that year) and 0 if not Create the vector fcst of forecast probabilities (i.e. number of ensemble members below the long-term observed Q90 threshold for that year, expressed as a probability [0,1]) In order to calculate the skill, we need a benchmark forecast against which to compare our ESP forecast A common benchmark forecast is the long-term climatology. In any given year the probability of the flow being below the Q90 threshold is simply 0.1 (i.e. the climatological probability). Therefore create the vector bench of climatological probabilities for each forecast (i.e. 0.1) The Brier score is then calculated using the brier() function from the verification R package library(verification) # Vector of binary observations obs_vec &lt;- ifelse(obs &lt; obs_Q90_threshold, 1, 0) # Vector of forecast probabilities (i.e. number of ensemble members below # threshold / total number of ensemble members) fcst_prob_vec &lt;- apply(fcst_ens &lt; obs_Q90_threshold, MARGIN = 1, FUN = mean) # Vector of climatological probabilities for benchmark forecast, simply a # vector 0.1 (i.e. below Q90 threshold 10% of the time by chance) clim_vec &lt;- rep(0.1, 50) # Calculate Brier Score for ESP forecast BS_esp &lt;- brier(obs = obs_vec, pred = fcst_prob_vec, bins = FALSE)$bs BS_esp ## [1] 0.06038447 # Calculate Brier Score for climatology benchmark forecast BS_bench &lt;- brier(obs = obs_vec, pred = clim_vec, bins = FALSE)$bs BS_bench ## [1] 0.09 Calculate the Brier Skill Score (BSS) We can compute the Brier Skill Score of the ESP forecast for predicting Q90 low flow events compared to a simple climatology benchmark forecast using the the generic skill score formula introduced in Section 12.3.8, with the following variables: Afc = BS_esp Abench = BS_bench Aperf = 0 # Brier Skill Score BSS &lt;- (BS_esp - BS_bench) / (0 - BS_bench) BSS ## [1] 0.3290615 Tobias: would use the function lfquantile() from the package. The names of the vector are easier to interpret.↩︎ "]]
