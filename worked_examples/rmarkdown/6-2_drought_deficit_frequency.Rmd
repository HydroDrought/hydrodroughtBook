---
title: "Drought deficit frequency analysis"
author: "Worked example 6.2"
---

```{r, setup, include=FALSE}
ggplot2::theme_set(ggplot2::theme_bw(base_size = 10))
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

River Ngaruroro at Kuripapango in New Zealand, applied for frequency
analysis of annual minimum series in Worked Example 6.1, is here applied
for frequency analysis of drought deficit characteristics. The drought events
were derived using the Nizowka program (Software, CD) as described in
Worked Example 5.4 (Section 5.4.1). Partial duration series (PDS) of drought
events below a given threshold were selected from time series of daily
discharge, and $Q_{90}$ was selected as the threshold. Two drought characteristics
are analysed in the following: drought deficit volume and real drought
duration as defined in Worked Example 5.4.

# Data
```{r}
library(tidyverse)
library(hydroDrought)
library(lubridate)

ngaruroro <- intl %>%
  filter(river == "Ngaruroro") %>%
  select(data) %>%
  unnest(data) %>%
  # fill gaps with a length of up to 15 days
  sanitize_ts(approx.missing = 15) %>%
  mutate(
    year = water_year(time, origin = "-09-01")
  ) 
```


```{r}
# remove incomplete years
coverage <- ngaruroro %>%
  filter(!is.na(discharge)) %>%
  pull(time) %>%
  coverage_yearly(origin = "-09-01")

incomplete <- coverage %>%
  filter(days.missing > 0) %>%
  print()

complete <- coverage %>%
  filter(days.missing == 0)

ngaruroro <- ngaruroro %>%
  anti_join(incomplete, by = "year")
```

```{r, echo=FALSE}
ng <- ngaruroro
```


Drought events were selected for River Ngaruroro and details of the
selection criteria are given in Worked Example 5.4. The series cover the
period `r min(ng$year)` to `r max(ng$year)` and the start of the year is set to 1 September. 


A total of `r hydroDrought:::numbers_english(nrow(incomplete))` years were omitted from the series, 
`r hydroDrought:::paste_with_and(paste(incomplete$year, substr(incomplete$year + 1, 3L, 4L), sep = "/"))`, 
due to missing data.

```{r}
q90 <- lfquantile(ngaruroro$discharge, exc.freq = 0.9) 

droughts <- ngaruroro %>%
  drought_events(
    threshold = q90, pooling = "inter-event", 
    pooling.pars = list(min.duration = 2, min.vol.ratio = Inf), 
    full.table = TRUE
  ) %>%
  mutate(
    year = water_year(first.day, origin = "-09-01"),
    #year = factor(year, levels = full_seq(ngaruroro$year, 1))
  )

tbl <- droughts %>%
  transmute(
    event, year, first.day, last.day, 
    duration = as.double(duration, units = "days"),
    real.duration = dbt, 
    volume = volume / 1e3,
    rel.volume = volume / (17.30839 * 86.4),
    # rel.volume = as.difftime(round(rel.volume, 2), unit = "days"), 
    average.deficit = volume / duration
  ) %>%
  print()
```


```{r echo=FALSE}
missing <- complete %>%
  anti_join(tbl, by = "year")
```

As only events below the $Q_{90}$
percentile are selected, it might happen that the flow never becomes less
than the threshold in a year (non-drought year). 

A total of `r hydroDrought:::numbers_english(length(missing))` out of the 
`r nrow(complete) - nrow(missing)` years with observations did not experience a drought 
(`r round(100 * nrow(missing) / nrow(complete), 1)`%). The PDS
series of drought deficit volume and real duration are plotted in
Figure 6.12. Less severe values are found in the second half of the
observation period for both deficit volume (upper) and duration (lower).
The data are still treated as one sample as the number of observations is
considered insufficient for a separate analysis of two periods. It should
further be noted that a similar trend towards less severe droughts is not as
pronounced for the $AM(1)$ values (Worked Example 6.1). This is likely a
result of the high base flow contribution in the catchment (Figure 6.6).


```{r, echo=FALSE}
fig.cap <- "Figure 6.12 PDS of drought deficit volume (upper) and real duration (lower) for River
Ngaruroro at Kuripapango (NZ)."
```
```{r, echo=FALSE, fig.cap=fig.cap}
library(cowplot)
p1 <- ggplot(tbl, aes(x = first.day, y = average.deficit)) +
  geom_point() + 
  scale_x_date(expand = expansion(mult = 0.01), date_minor_breaks = "1 year") +
  scale_y_continuous(limits = c(0, 200), expand = expansion()) + 
  labs(y = expression(paste("Average deficit volume (", 10^{3}, m^{3}, ")"))) +
  theme(axis.title.x = element_blank())

p2 <- ggplot(tbl, aes(x = first.day, y = as.double(real.duration))) +
  geom_point() + 
  scale_x_date(expand = expansion(mult = 0.01), date_minor_breaks = "1 year") +
  scale_y_continuous(limits = c(0, 70), expand = expansion()) +
  labs(y = "Real duration (days)") + 
  theme(axis.title.x = element_blank())

cowplot::plot_grid(p1, p2, align = "v", axis = "x", ncol = 1)
```



## Derivation of PDS and annual maxima series

^[Comment Tobias: Threshold of 10 is just a suggestion. This way the length of PDS and AMS do not differ very much. ]
```{r}
threshold <- 10
pds <- tbl %>%
  select(event, year, duration, first.day, last.day) %>%
  filter(duration > threshold) %>%
  print()
```


```{r}
ams <- tbl %>%
  select(event, year, duration, first.day, last.day) %>%
  group_by(year) %>%
  slice_max(duration, with_ties = FALSE) %>%
  right_join(complete %>% select(year), by = "year") %>%
  ungroup() %>%
  replace_na(replace = list(duration = 0)) %>%
  arrange(year) %>%
  print()
```

## Fitting 


in the [Journal of Statistical Software](https://www.jstatsoft.org/index.php/jss/article/view/v072i08/v72i08.pdf) there is an article describing the extremes package. 

Their approach is identical to Equation 6.11 in the first edition of the book. 

> The quantiles of the GP df are easily obtained by setting Equation 5 equal to
$1 - p$ and inverting. However, the quantiles of the GP df cannot be as readily interpreted as
return levels because the data no longer derive from specific blocks of equal length. Instead,
an estimate of the probability of exceeding the threshold, e.g., called $\zeta_u$ , is required. Then,
the value $x_m$ that is exceeded on average once every $m$ observations (i.e., an estimated return
level) is 

$$ x_m = u + \frac{\sigma_u}{\xi} \left[ (m \zeta_u)^\xi - 1\right] $$
with $m$ being the return period and $\zeta_u$ the overall exceedance rate (= average number of exceedances 
per year). So the return period is just multiplied with the exceedance rate. 
$u$ is the location parameter of the GPA (= threshold), $\sigma_u$ is scale and $\xi$ is shape.
In our case $\zeta_u = \frac{\text{length of PDS}}{\text{record length in years}}$. 

This transformation will introduce return periods of less than a year (or negative 
probabilities) for values $P_{PDS} < 1 + \zeta_u$. Return periods for PDS of less 
than a year just imply that such an event occurs on average several times a year. 

$$P_{annual} = 1 - \frac{(1 - P_{PDS})}{\zeta_u}$$

I've seen PDS return periods < 1 for example in [Return Periods of Hydrological Events, Rojsberg 1976](https://iwaponline.com/hr/article/8/1/57/1454/Return-Periods-of-Hydrological-EventsPaper). I've also looked at his approach of relating PDS and AMS. But for return periods > 10 years quantiles of the AMS and PDS are practically identical, irrespective of the exceedance rate.   

```{r, echo=FALSE}
# prob_annual <- function(prob.pds, events.per.year)
# {
#   x <- (1 - (1 - prob.pds) / events.per.year )^events.per.year
#   
#   bad <- 1 - prob.pds > events.per.year
#   if (any(bad)) {
#     warning("1 - prob.pds must be <= events.per.year")
#     x[bad] <- NA
#   }
#   
#   return(x)
# }

adjust_prob_pds <- function(prob, exc.per.year) {
  1 - (1 - prob)/exc.per.year
}

adjust_prob_pds_inv <- function(prob.adj, exc.per.year) {
  1 - (1 - prob.adj) * exc.per.year
}


fit_distr <- function(x, dist, bound = NULL, correct.prob = FALSE)
{
  if (!correct.prob) correct.prob <- 1
  library(lmom)
  zeros <- x == 0
  freq.zeros <- sum(zeros) / length(x)
  if(dist %in% c("gpa", "ln3", "wak", "wei")) {
    par <- match.fun(paste0("pel", dist))(samlmu(x[!zeros], nmom = 5), bound = bound)  
  } else {
    par <- match.fun(paste0("pel", dist))(samlmu(x[!zeros], nmom = 5))
  }
  
  cdf <- match.fun(paste0("cdf", dist))
  qua <- match.fun(paste0("qua", dist))
  
  
  .cdf <- function(x, cdf, par, zeros, correct.prob = 1)
  {
    freq.zeros <- sum(zeros) / length(x)
    prob <- numeric(length(x))
    p1 <- cdf(x[!zeros], par)
    prob[!zeros] <- p1 * (1 - freq.zeros) + freq.zeros
    prob[zeros] <- seq(0, freq.zeros, length.out = sum(zeros))
    
    prob <- adjust_prob_pds(prob, exc.per.year = correct.prob)
    return(prob)
  }
  
  .qua <- function(p, qua = qua, par, freq.zeros, correct.prob = 1)
  {
    quant <- numeric(length(p))
    zeros <- !is.na(p) & p <= freq.zeros
    p <- adjust_prob_pds(p, exc.per.year = correct.prob)
    quant[zeros] <- 0
    p1 <- (p[!zeros] - freq.zeros) / (1 - freq.zeros)
    quant[!zeros] <- qua(f = p1, para = par)
    return(quant)
  }
  
  list(
    cdf = function(x) .cdf(x, cdf = cdf, par = par, zeros = zeros, 
                           correct.prob = correct.prob),
    qua = function(x) .qua(x, qua = qua, par = par, 
                           freq.zeros = freq.zeros,
                           correct.prob = correct.prob),
    par = par, 
    zeros = zeros)
}
```


```{r, echo=FALSE}
data <- tibble(
  series = c("AMS", "PDS", "PDS-corrected"),
  events.per.year = c(1, 1, nrow(pds) / nrow(complete)),
  bound = list(NULL, threshold, threshold),
  distribution = c("gum", "gpa", "gpa"),
  observations = list(ams$duration, 
                      pds$duration, 
                      pds$duration)
) %>%
  mutate(
    series = paste0(series, " (", distribution, ")"),
    observations = map2(observations, events.per.year, 
                        ~tibble(
                          value = .x, 
                          prob.emp = adjust_prob_pds_inv(rank(value, ties.method = "random") / (length(value) + 1), .y),
                          rp.emp = (length(value) + 1) / rank(-value)  * .y
                        )
    ),
    fit = pmap(list(x = map(observations, "value"), 
                    dist = distribution, 
                    bound = bound,
                    correct.prob = events.per.year),
               fit_distr),
    fitted = map(fit, ~tibble(
      prob = seq(0.01, 1, 0.005),
      value = .x$qua(prob)
    )),
    quantiles = map(fit, ~tibble(
      rp = c(20, 50, 75, 100, 200),
      prob = 1 - 1/rp,
      value = .x$qua(prob)
    ))
  )
```

The following plots are just to demonstrate what is going on. Using a Gumbel 
distribution for a conditional probability model is probably a poor choice because 
we cannot specify a lower bound. 

```{r, echo=FALSE, fig.cap="Linear probability scale on x-Axis. "}
reduced_var <- function (x) -log(1 - x)

obs <- data %>%
  select(series, observations) %>%
  unnest(observations) 

fitted <- data %>%
  select(series, fitted) %>%
  unnest(fitted) 

rp.breaks <- c(1:5, 10, 100)

ggplot(fitted, aes(y = value, col = series)) + 
  geom_line(aes(x = (prob))) + 
  geom_point(aes(x = (prob.emp)), data = obs, size = 0.5) +
  geom_vline(xintercept = (1 - 1/breaks_log10_all(c(1, 100))), 
             col = "lightgrey", size = 0.1) + 
  scale_x_continuous(
    expand = expansion(c(0.04, 0)), 
    sec.axis = sec_axis(
      trans = function(x) 1 / ( 1 - x), 
      breaks = rp.breaks,
      name = "Return period (in years)")
  ) + 
  labs(x = "Probability ? F(x)",
       y = expression(paste("Quantile (", m^{3}, s^{-1}, ")"))) + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        legend.position = c(0, 1), 
        legend.justification = c(-0.2, 1.2))
```


```{r, echo=FALSE, fig.cap="Same plot as above but with reduced variate."}


rp.breaks <- c(1:8, seq(10, 80, 10), 100, 150, 200)

ggplot(fitted, aes(y = value, col = series)) + 
  geom_line(aes(x = reduced_var(prob))) + 
  geom_point(aes(x = reduced_var(prob.emp)), data = obs, size = 0.5) +
  geom_vline(xintercept = reduced_var(1 - 1/breaks_log10_all(c(1, 100))),  
             col = "lightgrey", size = 0.1) + 
  scale_x_continuous(
    expand = expansion(c(0.04, 0)), 
    sec.axis = sec_axis(
      trans = function(x) 1/ (1 - (-exp(-x) + 1)), 
      breaks = rp.breaks,
      name = "Return period (in years)")
  ) + 
  labs(x = expression(paste("Exponential reduced variate -log(", 1, " - F(", x, "))")),
       y = expression(paste("Quantile (", m^{3}, s^{-1}, ")"))) + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        legend.position = c(0, 1), 
        legend.justification = c(-0.2, 1.2))
```


# Derivation of distribution function
Following Zelenhasic & Salvai (1987) an estimate of the non-exceedance
probability, $F(x)$, for the largest event in each time interval is in Nizowka
obtained by combining the distribution for the occurrence of events and
the distribution for the magnitudes of deficit volume or duration. Here a
time interval of one year is chosen. Subsequently the return period in
years for a given event can be calculated. The number of drought events
occurring in a time period t is commonly assumed to be Poisson
distributed (Equation 6.9) with parameter $\lambda t$. In Nizowka the binomial
Pascal distribution is offered along with the Poisson distribution as
described in ‘Background Information NIZOWKA’ (Software, CD). The
distribution that best fitted deficit volume was the Pascal distribution for
the number of droughts and the GP distribution for the deficits. For
duration the Pascal distribution was chosen along with the Log-Normal
distribution. The $F(x)$ is for drought deficit volume plotted in Figure 6.13
together with the observed values plotted using a plotting position. The
chosen distribution describes the data well, with the exception of some
values in the upper range. The maximum value is, however, satisfactorily
modelled.



```{r, echo=FALSE}
library(lmom)
rp <- c(20, 50, 75, 100, 200)
p <- 1 - c(1 / rp)

thresh <- tribble( 
  ~metric, ~threshold,
  "real.duration", 10,
  "abs.volume", 100
)

observations <- tbl %>% 
  select(real.duration, abs.volume = volume) %>%
  mutate(across(everything(), as.numeric)) %>%
  pivot_longer(cols = everything(), names_to = "metric") %>%
  nest(data = value)  %>%
  left_join(thresh, by = "metric") %>%
  mutate(pds = map2(data, threshold, ~tibble(value = .x[.x>.y])))

fitted <- observations %>% 
  mutate(
    rate = map2_dbl(pds, data, ~(nrow(.x) / nrow(complete))), 
    parameter = map(pds, ~pelgpa(samlmu(.x$value))),
    quantiles = map2(parameter, rate, ~tibble(
      p = p, 
      p.adj = adjust_prob_pds(p, .y),
      quant = quagpa(p.adj, .x))
    ),
    quafun = map2(parameter, rate, ~tibble(
      p = seq(0.01, 0.991, 0.001),
      p.adj = adjust_prob_pds(p, .y),
      quant = quagpa(p.adj, .x)
      ))
  )
```


```{r echo=FALSE, fig.height=8}
reduced_var <- function (x) -log(1 - x)

o <- fitted %>%
  mutate(
    metric = factor(
      metric, levels = c("real.duration", "abs.volume"),
      labels = c("Real duration\n(in days)", 
                 "Absolute volume\n(in 1000 m³)"))
  ) %>%
  unnest(pds) %>%
  group_by(metric) %>%
  mutate(prob = adjust_prob_pds_inv(rank(value) / (n() + 1), rate[1])) 

f <- fitted %>%
  mutate(
    metric = factor(
      metric, levels = c("real.duration", "abs.volume"),
      labels = c("Real duration\n(in days)", 
                 "Absolute volume\n(in 1000 m³)"))
  ) %>%
  select(metric, quafun) %>%
  unnest(quafun)

rp.breaks <- c(1:8, seq(10, 80, 10), 100, 150, 200)

ggplot(o, aes(x = reduced_var(prob), y = value)) + 
  geom_vline(xintercept = reduced_var(1 - 1/breaks_log10_all(c(1, 100))),
             col = "lightgrey", size = 0.1) + 
  geom_point(size = 0.5) + 
  geom_line(data = f, aes(x = reduced_var(p), y = quant), size = 0.2) + 
  scale_x_continuous(
    expand = expansion(c(0.04, 0)), 
    sec.axis = sec_axis(
      trans = function(x) 1/ (1 - (-exp(-x) + 1)), 
      breaks = c(1, seq(10, 80, 10), 100, 150, 200),
      name = "Return period (in years)")
  ) + 
  facet_wrap(vars(metric), scales = "free_y", 
             ncol = 1, strip.position = "left") +
  labs(x = expression(paste("Exponential reduced variate -log(", 1, " - F(", x, "))")),
       y = "Quantile") + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
```


# Calculation of the T-year event
The return period of a given event is calculated following Equation 6.4.
The relationship between the drought characteristics as defined in Worked
Example 5.4 and $F(x)$ are given by the tabulated distribution functions in
Nizowka. The design value for a particular return period, i.e. the T-year
event, can be obtained from the tables for known values of $F(x)$. The
estimated 10-, 100- and 200-year drought events are shown in Table 6.2.


```{r}
fitted %>%
  select(metric, quantiles) %>%
  unnest(quantiles)
```


```{r, echo=FALSE}
# fitted  %>%
#   select(metric, quantiles) %>%
#   unnest(quantiles) %>%
#   pivot_wider(id_cols = p, names_from = "metric", values_from = "quant") %>%
#   mutate(return.period = 1 / (1 - p)) %>%
#   select(prob = p, return.period, everything())
```



# extRemes package (with bootstrapped confidence intervals)

```{r}
# the drought durations
x <- as.numeric(tbl$duration)
```


Unfortunately we cannot suggest the use of **extRemes** package directly 
because it doesn't fit distributions to already derived PDS. It computes the
PDS internally but doesn't allow for a specification of initial record length.  

## Hack 1
Hack 1 would be to extend the time series of drought durations with zeros to match 
the length of the discharge time series. This way the internal procedure computes 
excess rate correctly. 

```{r}
xx <- rep(0, round(nrow(complete) * 365.25))
xx[seq_along(x)] <- x
head(xx, 500)

rp <- c(20, 50, 75, 100, 200)

library(extRemes)
fit <- fevd(xx, threshold = threshold, type = "GP", method = "Lmoments",
            time.units = "days")

ci(fit, return.period = rp)
```


## Hack 2
Hack 2 would be to alter the object created by `fevd()` to use the correct rate 
$\zeta_u$. 

```{r}
rate <- sum(x > threshold) / nrow(complete)
fit <- fevd(x, threshold = threshold, type = "GP", method = "Lmoments",
            time.units = "years")
fit$rate <- rate
ci(fit, return.period = rp)
```

# Manual computation (without confidence intervals)

```{r}
library(lmom)

adjust_prob_pds <- function(prob, exc.per.year) {
  1 - (1 - prob)/exc.per.year
}

prob <- 1 - 1/rp
parameter <- pelgpa(samlmu(x[x > threshold]), bound = threshold)
quagpa(adjust_prob_pds(prob, rate), parameter)

```

