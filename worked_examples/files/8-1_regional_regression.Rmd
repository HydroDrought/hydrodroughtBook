---
title: "Regional Regression"
author: "Worked example 8.1"
output: 
  tufte::tufte_html:
    tufte_features: ["fonts"]
    css: style.css
    number_sections: true
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
theme_set(theme_bw(base_size = 10))
library(kableExtra)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)

# str() doesn't respect the options(width = ...)
str <- function(...) utils::str(..., width = 75, strict.width = "cut")
```



# Introduction
This worked example^[Tobias: Will be adapted soon...] demonstrates the application of regional regression analysis to model low flows in a spatial context. We use a data set of 30 catchments situated in the forelands and prealps in the north-east of Austria (Zone 1). The data set is fully described in Laaha & BlÃ¶schl (2006a, 2006b).

The target variable is the long-term low flow characteristic $Q95$ (flow quantile, that is exceeded 95% of the time) , standardized by catchment area to eliminate its predominant effect on low flows. For nested catchments, the $Q95$ was dis-aggregated into the residual $Q95$ of sub-catchments, assuming regional synchronicity of low flow events. 

In this section we learn how to

* fit a multiple linear regression model, 
* perform stepwise regression to deal with collinearity, 
* employ the Cook's distance method to deal with outliers, 
* use robust regression methods and 
* create specific plots to analyse the quality of the regression model and the specific contribution of each predictor to the regression estimate.


# Data import
The text file named `zone1_30_ENG.txt` contains several catchment descriptors for the 30 catchments of the regional data set mentioned above. Each observation (each line) in the text file corresponds to a single gauging station located at the catchment outlet. The variables (our catchment descriptors) are represented as columns of this data set. 

Let us import this tabular data set into an object `x0`. As the vary first line of the  text file contains the column headers, we have to specify the `header` argument accordingly. The function `read.table()` will return a `data.frame`. 

```{r}
x0 <- read.table("../../../hd-book/chapter8/Regression/zone1_30_ENG.txt", header = TRUE)
```


To get a glimpse of the imported data set, let us print the first three lines of the object `x0`. They contain the catchment descriptors of the first three catchments. 

```{r}
head(x0, n = 3)
```

To make sure all variables are imported either as class `integer` or `numeric` (and not accidentally as class `character` or `factor`) let us have a look at the structure of the object `x0`. Calling the function `str()` will print the dimensions of the `data.frame` in the very first line. The following lines contain the name of the variable, the class of the variable and the first few values. 

```{r}
str(x0)
```


Another way to find out the dimensions of an object is to call `dim(x0)`, `ncol(x0)` or `nrow(x0)` explicitly. 

The raw data set `x0` contains some variables which we do not want to use in our regression model, e.g. the id number or the coordinates of the gauging station. We will remove the columns to simplify the subsequent analysis. It is also a good idea to assign meaningful row names to the `data.frame`  (e.g. the station id `'HZBNR'`) to be able to identify the observations later on. 


```{r}
rownames(x0) <- x0$HZBNR

remove <- c("DBMSNR", "HZBNR", "X_LB", "Y_LB", "Q95s", "Q95w", "A")
keep <- setdiff(colnames(x0), remove)
x <- x0[, keep]
head(x)
```


# Simple linear regression

```{r, fig.height=6}
lm.simple <- lm(Q95 ~ P, data = x)

summary(lm.simple)
par(mfrow = c(2, 2))
plot(lm.simple)
```


<!-- Explore the fitted model object -->
<!-- ```{r} -->
<!-- str(lm.simple) -->
<!-- ``` -->
<!-- Note that the object is a list, containing vectors, data frames, etc. as elements.  -->
<!-- These elements may be extracted for further analysis via, e.g.`lm.simple$residuals` -->

<!-- ```{r} -->
<!-- lm.simple$residuals -->
<!-- ``` -->



# Stepwise regression
In stepwise regression an 'optimal' model (based on the Akaike Information Criterion, $AIC$) is obtained by iteratively adding or removing variables. 


## Manual backward selection
When starting from the full model (all variables are used as predictors) the function `drop1()` can be used to remove the least significant variable. If the resulting model is considered better if its $AIC$ is lower than the original model. This procedure is called *backward selection*. 

```{r}
model.full <- lm(Q95 ~ ., data = x)
```

The model is specified by a so called *formula*. It has to parts, a left-hand side (LHS) and a right-hand side (RHS), separated by a tilde sign '~'. The LHS contains just the name of the variable we want to predict with our model, in our its the $Q95$. On the RHS we have to list all variables that should be used as predictors in the model. There is no need  of mentioning all variables explicitly. In this context the dot ('.') means *all columns of the data argument not otherwise used in the formula*. Hence `Q95 ~ .` is equivalent to 

```{r, echo=FALSE}
formula(Q95 ~ H0 + Hx + Hr + Hm + Sm + Ssl + Smo + Sst + P + Ps + Pw + Gb + Gq + Gt + Gf + Gl + Gc + Ggs + Ggd + Gso + Lu + La + Lc + Lg + Lf + Lr + Lwa + D)
```

```{r}
drop1(model.full)
```


```{r include=FALSE}
tab <- drop1(model.full)
best <- head(tab[order(tab$AIC, decreasing = TRUE), ], 1)
```

The above table tells us that among all single term deletions (removing only one variable) the model without $`r rownames(best)`$ would have the worst (=highest) sum of squared residuals (RSS) value of ``r round(best$RSS, 1)`` and therefore also the worst (=highest) the AIC of ``r round(best$AIC, 2)``.


## Manual forward selection
Another approach is to start from the empty model and adding the variable with highest explicative value by applying the function `add1()`. This is called *forward selection*. When using the function `drop1()`, there was no need to explicitly define the scope of the variables to drop because for `drop1()` a missing scope is taken to be all terms in the model. That is not the case for its counterpart `add1()`; here we have to set the scope explicitly. 

In our case the scope contains all variables. And instead of using an empty model our initial model already contains the xxx precipitation $P$, as we can safely assume that the $Q95$ low flow will depend on the xxx precipitation sum of the catchment. 

```{r}
model.init <- lm(Q95 ~ P, data = x)
scope <- formula(paste("~", paste(colnames(x)[-1], collapse = " + ")))
add1(model.init, scope = scope)
```


```{r include=FALSE, results=FALSE}
tab <- add1(model.init, scope = scope)
best <- head(tab[order(tab$AIC), ], 1)
```

Again, this table tells us that among all other variables choosing the variable $`r rownames(best)`$ would minimize the sum of squared residuals (RSS) to a value of ``r round(best$RSS, 1)`` and the AIC to ``r round(best$AIC, 2)``.


## Automated variable selection
The process described above can be repeated unless any removal or addition of a variable would result in an increase of the $AIC$ and therefore in an inferior model. 

```{r}
lm.forward <- step(model.init, scope = scope, direction = "forward", trace = 0) 
summary(lm.forward)
```

It is also possible to obtain an ANOVA-like F-test of predictor significance (i.e. sequential ANOVA) by calling the `anova()` method on the object return by `step()`. 
```{r}
anova(lm.forward)

par(mfrow = c(2, 2))
plot(lm.forward)
```



```{r}
lm.backward <- step(model.full, scope = scope, direction = "backward", trace = 0) 
summary(lm.backward)
```

```{r, fig.height=6}
lm.both <- step(model.init, scope = scope, direction = "both", trace = 0) 
summary(lm.both)

par(mfrow = c(2,2))
plot(lm.both)
```




## Plotting the results

One can show the model performance in a scatter plot by printing the predicted values of the Q95 floe against the actual, observed values of the Q95. 

Macht man in der Regel nicht, oder? Eher resid vs. obs. 
 
```{r}
plot(x = x$Q95, y = resid(lm.both), xlab = "observed Q95", ylab = "residuals")

plot(x = x$Q95, y = fitted(lm.both), xlab = "observed Q95", ylab = "predicted")
abline(lsfit(lm.both$model$Q95, lm.both$fitted.values))
```


To depict the contribution of each predictor $x_j$ used in the model to the estimated $y$  the function `termplot()` plots each regression term ($\beta_j \cdot x_j$) against its predictor.


```{r, fig.height=6}
par(mfrow = c(2,2))
termplot(lm.both, partial.resid = TRUE) 
```

# Remove outliers based on Cook's distance

```{r include=FALSE}
cooks <- sort(abs(cooks.distance(lm.both)), decreasing = TRUE)
```


In the diagnostic plot of the final model obtained by stepwise regression the station with id number ``r names(cooks)[1]`` popped out because of its high Cook's distance of ``r round(cooks[1], 2)``. 

The analysis is conducted in 3 steps (that may be repeated until no outliers remain...)

(@) Detect outliers based on Cook's distance. You can do this either by looking a the diagnostic plot or by subsetting the result of the function `cooks.distance()`. Let's for example eliminate all outliers with an absolute Cook's distance larger than 1.
```{r}
pos <- which(abs(cooks.distance(lm.both)) >= 1)
pos
```
 
In our data set this is the station with the id number ``r names(pos)``, which is observation number (row number) ``r pos``.
 

(@) Now we can eliminate the outliers manually, by removing this row. 
```{r}
x1 <- x[-pos, ]
dim(x1)
```

(@) Finally we have to re-run our stepwise regression model. 

```{r}
lm.new <- step(lm(Q95 ~ P, data = x1), scope = scope, trace = 0) 
summary(lm.new)

par(mfrow = c(2,2))
plot(lm.new)
```



# Robust regression (alternative to manual outlier detection)

```{r, warning=FALSE}
library(robustbase)
set.seed(3141) # to set seed for random number generator
model <- Q95 ~ Ggs + Gc + Sm + H0
x.lts <- ltsReg(formula = model, data = x, alpha = 0.9)
summary(x.lts)
plot(x.lts)


x.rob <- lmrob(formula = model, data = x)
summary(x.rob)
plot(x.rob)
```



# References
* Laaha, G. & BlÃ¶schl, G. (2006a) Seasonality indices for regionalizing low flows. Hydrol. Process. 20(18), 3851-3878. doi:10.1002/hyp.6161
* Laaha, G. & BlÃ¶schl, G. (2006b) A comparison of low flow regionalisation methods-catchment grouping. J. Hydrol. 323(1-4), 193-214. doi:10.1016/j.jhydrol.2005.09.001
